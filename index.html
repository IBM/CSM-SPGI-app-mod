
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="IBM CSM SPGI Application Modernisation on Red Hat OpenShift Workshop">
      
      
      
      
      
      
      <link rel="icon" href="csm-icon.png">
      <meta name="generator" content="mkdocs-1.5.3, mkdocs-material-9.5.14">
    
    
      
        <title>IBM CSM SPGI Application Modernisation Workshop</title>
      
    
    
      <link rel="stylesheet" href="assets/stylesheets/main.10ba22f1.min.css">
      
        
        <link rel="stylesheet" href="assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=IBM+Plex+Sans:300,300i,400,400i,700,700i%7CIBM+Plex+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"IBM Plex Sans";--md-code-font:"IBM Plex Mono"}</style>
      
    
    
      <link rel="stylesheet" href="css/timeago.css">
    
      <link rel="stylesheet" href="css/extra.css">
    
      <link rel="stylesheet" href="css/csm-spgi.css">
    
      <link rel="stylesheet" href="css/tables.css">
    
    <script>__md_scope=new URL(".",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="black" data-md-color-accent="blue">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#application-modernization-on-red-hat-openshift-technical-workshop" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow md-header--lifted" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="." title="IBM CSM SPGI Application Modernisation Workshop" class="md-header__button md-logo" aria-label="IBM CSM SPGI Application Modernisation Workshop" data-md-component="logo">
      
  <img src="csm-icon.png" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            IBM CSM SPGI Application Modernisation Workshop
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Home
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme="default" data-md-color-primary="black" data-md-color-accent="blue"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 6H7c-3.31 0-6 2.69-6 6s2.69 6 6 6h10c3.31 0 6-2.69 6-6s-2.69-6-6-6zm0 10H7c-2.21 0-4-1.79-4-4s1.79-4 4-4h10c2.21 0 4 1.79 4 4s-1.79 4-4 4zM7 9c-1.66 0-3 1.34-3 3s1.34 3 3 3 3-1.34 3-3-1.34-3-3-3z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme="slate" data-md-color-primary="blue" data-md-color-accent="blue"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5m0 8a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3Z"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var media,input,key,value,palette=__md_get("__palette");if(palette&&palette.color){"(prefers-color-scheme)"===palette.color.media&&(media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']"),palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent"));for([key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
    
  </nav>
  
    
      
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
    
  
  
    <li class="md-tabs__item md-tabs__item--active">
      <a href="." class="md-tabs__link">
        
  
    
  
  Home

      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
    
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="." title="IBM CSM SPGI Application Modernisation Workshop" class="md-nav__button md-logo" aria-label="IBM CSM SPGI Application Modernisation Workshop" data-md-component="logo">
      
  <img src="csm-icon.png" alt="logo">

    </a>
    IBM CSM SPGI Application Modernisation Workshop
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    Home
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="." class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    Home
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#lab-1-deploy-and-expose-an-application-using-red-hat-openshift-console" class="md-nav__link">
    <span class="md-ellipsis">
      Lab 1 - Deploy and expose an application using Red Hat OpenShift console.
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Lab 1 - Deploy and expose an application using Red Hat OpenShift console.">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-login-to-the-console" class="md-nav__link">
    <span class="md-ellipsis">
      1. Login to the console
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-deploy-a-new-application" class="md-nav__link">
    <span class="md-ellipsis">
      2. Deploy a new application
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lab-2-deploy-and-expose-an-application-using-red-hat-openshift-command-line" class="md-nav__link">
    <span class="md-ellipsis">
      Lab 2 - Deploy and expose an application using Red Hat OpenShift command line
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Lab 2 - Deploy and expose an application using Red Hat OpenShift command line">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-login-with-the-command-line" class="md-nav__link">
    <span class="md-ellipsis">
      1. Login with the command line
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-deploy-a-new-application_1" class="md-nav__link">
    <span class="md-ellipsis">
      2. Deploy a new application
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-expose-the-application" class="md-nav__link">
    <span class="md-ellipsis">
      3. Expose the application
    </span>
  </a>
  
    <nav class="md-nav" aria-label="3. Expose the application">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#31-cluster-ip" class="md-nav__link">
    <span class="md-ellipsis">
      3.1 Cluster IP
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#33-ingress" class="md-nav__link">
    <span class="md-ellipsis">
      3.3 Ingress
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#4-clean-all-the-artifacts" class="md-nav__link">
    <span class="md-ellipsis">
      4. Clean all the artifacts
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lab-3-storage" class="md-nav__link">
    <span class="md-ellipsis">
      Lab 3 - Storage
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Lab 3 - Storage">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-check-your-current-application" class="md-nav__link">
    <span class="md-ellipsis">
      1. Check your current application
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-storage-classes" class="md-nav__link">
    <span class="md-ellipsis">
      2. Storage Classes
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-create-a-persistent-volume-claim" class="md-nav__link">
    <span class="md-ellipsis">
      3. Create a Persistent Volume Claim
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#4-use-the-persistence-volume-claim-in-an-application" class="md-nav__link">
    <span class="md-ellipsis">
      4. Use the persistence volume claim in an application
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#5-clean-all-the-artifacts" class="md-nav__link">
    <span class="md-ellipsis">
      5. Clean all the artifacts
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lab-4-image-repository" class="md-nav__link">
    <span class="md-ellipsis">
      Lab 4 - Image repository
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Lab 4 - Image repository">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-create-an-image-nodejs" class="md-nav__link">
    <span class="md-ellipsis">
      1. Create an image (Node.js)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-create-an-image-java-ee" class="md-nav__link">
    <span class="md-ellipsis">
      2. Create an image (Java EE)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-publish-the-image-to-openshift" class="md-nav__link">
    <span class="md-ellipsis">
      3. Publish the image to OpenShift
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#4-create-a-deployment" class="md-nav__link">
    <span class="md-ellipsis">
      4. Create a Deployment
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#5-clean-all-the-artifacts_1" class="md-nav__link">
    <span class="md-ellipsis">
      5. Clean all the artifacts
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lab-5-application-management" class="md-nav__link">
    <span class="md-ellipsis">
      Lab 5 - Application Management
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Lab 5 - Application Management">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-automatic-restarts" class="md-nav__link">
    <span class="md-ellipsis">
      1. Automatic restarts
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-query-application-health" class="md-nav__link">
    <span class="md-ellipsis">
      2. Query application health
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-manual-scaling" class="md-nav__link">
    <span class="md-ellipsis">
      3. Manual scaling
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#4-automatic-scaling" class="md-nav__link">
    <span class="md-ellipsis">
      4. Automatic scaling
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#5-clean-all-the-artifacts_2" class="md-nav__link">
    <span class="md-ellipsis">
      5. Clean all the artifacts
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lab-6-network-policies" class="md-nav__link">
    <span class="md-ellipsis">
      Lab 6 - Network Policies
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Lab 6 - Network Policies">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-create-two-deployments-and-create-a-route" class="md-nav__link">
    <span class="md-ellipsis">
      1. Create two deployments and create a route
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-verify-access-to-pod" class="md-nav__link">
    <span class="md-ellipsis">
      2. Verify access to pod
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-create-the-deny-all-network-policy" class="md-nav__link">
    <span class="md-ellipsis">
      3. Create the deny-all network policy
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#4-create-a-network-policy-to-allow-traffic" class="md-nav__link">
    <span class="md-ellipsis">
      4. Create a network policy to allow traffic
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#5-clean-all-the-artifacts_3" class="md-nav__link">
    <span class="md-ellipsis">
      5. Clean all the artifacts
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lab-7-operators" class="md-nav__link">
    <span class="md-ellipsis">
      Lab 7 - Operators
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Lab 7 - Operators">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-deploy-an-operator-using-the-operators-catalogues" class="md-nav__link">
    <span class="md-ellipsis">
      1. Deploy an operator using the Operators Catalogues
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-deploy-a-service-instance-using-the-operator" class="md-nav__link">
    <span class="md-ellipsis">
      2. Deploy a service instance using the operator
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-clean-all-the-artifacts" class="md-nav__link">
    <span class="md-ellipsis">
      3. Clean all the artifacts
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lab-8-source2image-s2i" class="md-nav__link">
    <span class="md-ellipsis">
      Lab 8 - Source2Image (S2I)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Lab 8 - Source2Image (S2I)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-create-the-image-from-the-git-repository" class="md-nav__link">
    <span class="md-ellipsis">
      1. Create the image from the Git repository
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-clean-all-the-artifacts" class="md-nav__link">
    <span class="md-ellipsis">
      2. Clean all the artifacts
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lab-9-logging" class="md-nav__link">
    <span class="md-ellipsis">
      Lab 9 - Logging
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Lab 9 - Logging">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-command-line" class="md-nav__link">
    <span class="md-ellipsis">
      1. Command line
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-openshift-console" class="md-nav__link">
    <span class="md-ellipsis">
      2. OpenShift Console
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-kibana" class="md-nav__link">
    <span class="md-ellipsis">
      3. Kibana
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lab-10-monitoring" class="md-nav__link">
    <span class="md-ellipsis">
      Lab 10 - Monitoring
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Lab 10 - Monitoring">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-dashboards" class="md-nav__link">
    <span class="md-ellipsis">
      1. Dashboards
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-alerts" class="md-nav__link">
    <span class="md-ellipsis">
      2. Alerts
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-metrics" class="md-nav__link">
    <span class="md-ellipsis">
      3. Metrics
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lab-11-cicd-with-pipelines-tekton-and-argocd" class="md-nav__link">
    <span class="md-ellipsis">
      Lab 11 - CI/CD with Pipelines Tekton and ArgoCD
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Lab 11 - CI/CD with Pipelines Tekton and ArgoCD">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#continuous-integration-ci" class="md-nav__link">
    <span class="md-ellipsis">
      Continuous Integration (CI)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#continuous-delivery-cd" class="md-nav__link">
    <span class="md-ellipsis">
      Continuous Delivery (CD)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#deploy-an-application-using-ci-pipelines-with-tekton" class="md-nav__link">
    <span class="md-ellipsis">
      Deploy an Application using CI Pipelines with Tekton
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#promote-an-application-using-cd-with-gitops-gitea-config-repo-and-argocd" class="md-nav__link">
    <span class="md-ellipsis">
      Promote an Application using CD with GitOps (gitea config repo) and ArgoCD
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lab-12-application-modernization-with-transformation-advisor" class="md-nav__link">
    <span class="md-ellipsis">
      Lab 12 - Application modernization with Transformation Advisor
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lab-13-scale-your-application-with-gitops-and-argocd" class="md-nav__link">
    <span class="md-ellipsis">
      Lab 13 - Scale your application with GitOps and ArgoCD
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#lab-1-deploy-and-expose-an-application-using-red-hat-openshift-console" class="md-nav__link">
    <span class="md-ellipsis">
      Lab 1 - Deploy and expose an application using Red Hat OpenShift console.
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Lab 1 - Deploy and expose an application using Red Hat OpenShift console.">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-login-to-the-console" class="md-nav__link">
    <span class="md-ellipsis">
      1. Login to the console
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-deploy-a-new-application" class="md-nav__link">
    <span class="md-ellipsis">
      2. Deploy a new application
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lab-2-deploy-and-expose-an-application-using-red-hat-openshift-command-line" class="md-nav__link">
    <span class="md-ellipsis">
      Lab 2 - Deploy and expose an application using Red Hat OpenShift command line
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Lab 2 - Deploy and expose an application using Red Hat OpenShift command line">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-login-with-the-command-line" class="md-nav__link">
    <span class="md-ellipsis">
      1. Login with the command line
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-deploy-a-new-application_1" class="md-nav__link">
    <span class="md-ellipsis">
      2. Deploy a new application
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-expose-the-application" class="md-nav__link">
    <span class="md-ellipsis">
      3. Expose the application
    </span>
  </a>
  
    <nav class="md-nav" aria-label="3. Expose the application">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#31-cluster-ip" class="md-nav__link">
    <span class="md-ellipsis">
      3.1 Cluster IP
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#33-ingress" class="md-nav__link">
    <span class="md-ellipsis">
      3.3 Ingress
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#4-clean-all-the-artifacts" class="md-nav__link">
    <span class="md-ellipsis">
      4. Clean all the artifacts
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lab-3-storage" class="md-nav__link">
    <span class="md-ellipsis">
      Lab 3 - Storage
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Lab 3 - Storage">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-check-your-current-application" class="md-nav__link">
    <span class="md-ellipsis">
      1. Check your current application
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-storage-classes" class="md-nav__link">
    <span class="md-ellipsis">
      2. Storage Classes
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-create-a-persistent-volume-claim" class="md-nav__link">
    <span class="md-ellipsis">
      3. Create a Persistent Volume Claim
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#4-use-the-persistence-volume-claim-in-an-application" class="md-nav__link">
    <span class="md-ellipsis">
      4. Use the persistence volume claim in an application
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#5-clean-all-the-artifacts" class="md-nav__link">
    <span class="md-ellipsis">
      5. Clean all the artifacts
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lab-4-image-repository" class="md-nav__link">
    <span class="md-ellipsis">
      Lab 4 - Image repository
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Lab 4 - Image repository">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-create-an-image-nodejs" class="md-nav__link">
    <span class="md-ellipsis">
      1. Create an image (Node.js)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-create-an-image-java-ee" class="md-nav__link">
    <span class="md-ellipsis">
      2. Create an image (Java EE)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-publish-the-image-to-openshift" class="md-nav__link">
    <span class="md-ellipsis">
      3. Publish the image to OpenShift
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#4-create-a-deployment" class="md-nav__link">
    <span class="md-ellipsis">
      4. Create a Deployment
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#5-clean-all-the-artifacts_1" class="md-nav__link">
    <span class="md-ellipsis">
      5. Clean all the artifacts
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lab-5-application-management" class="md-nav__link">
    <span class="md-ellipsis">
      Lab 5 - Application Management
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Lab 5 - Application Management">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-automatic-restarts" class="md-nav__link">
    <span class="md-ellipsis">
      1. Automatic restarts
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-query-application-health" class="md-nav__link">
    <span class="md-ellipsis">
      2. Query application health
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-manual-scaling" class="md-nav__link">
    <span class="md-ellipsis">
      3. Manual scaling
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#4-automatic-scaling" class="md-nav__link">
    <span class="md-ellipsis">
      4. Automatic scaling
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#5-clean-all-the-artifacts_2" class="md-nav__link">
    <span class="md-ellipsis">
      5. Clean all the artifacts
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lab-6-network-policies" class="md-nav__link">
    <span class="md-ellipsis">
      Lab 6 - Network Policies
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Lab 6 - Network Policies">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-create-two-deployments-and-create-a-route" class="md-nav__link">
    <span class="md-ellipsis">
      1. Create two deployments and create a route
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-verify-access-to-pod" class="md-nav__link">
    <span class="md-ellipsis">
      2. Verify access to pod
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-create-the-deny-all-network-policy" class="md-nav__link">
    <span class="md-ellipsis">
      3. Create the deny-all network policy
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#4-create-a-network-policy-to-allow-traffic" class="md-nav__link">
    <span class="md-ellipsis">
      4. Create a network policy to allow traffic
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#5-clean-all-the-artifacts_3" class="md-nav__link">
    <span class="md-ellipsis">
      5. Clean all the artifacts
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lab-7-operators" class="md-nav__link">
    <span class="md-ellipsis">
      Lab 7 - Operators
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Lab 7 - Operators">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-deploy-an-operator-using-the-operators-catalogues" class="md-nav__link">
    <span class="md-ellipsis">
      1. Deploy an operator using the Operators Catalogues
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-deploy-a-service-instance-using-the-operator" class="md-nav__link">
    <span class="md-ellipsis">
      2. Deploy a service instance using the operator
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-clean-all-the-artifacts" class="md-nav__link">
    <span class="md-ellipsis">
      3. Clean all the artifacts
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lab-8-source2image-s2i" class="md-nav__link">
    <span class="md-ellipsis">
      Lab 8 - Source2Image (S2I)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Lab 8 - Source2Image (S2I)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-create-the-image-from-the-git-repository" class="md-nav__link">
    <span class="md-ellipsis">
      1. Create the image from the Git repository
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-clean-all-the-artifacts" class="md-nav__link">
    <span class="md-ellipsis">
      2. Clean all the artifacts
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lab-9-logging" class="md-nav__link">
    <span class="md-ellipsis">
      Lab 9 - Logging
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Lab 9 - Logging">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-command-line" class="md-nav__link">
    <span class="md-ellipsis">
      1. Command line
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-openshift-console" class="md-nav__link">
    <span class="md-ellipsis">
      2. OpenShift Console
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-kibana" class="md-nav__link">
    <span class="md-ellipsis">
      3. Kibana
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lab-10-monitoring" class="md-nav__link">
    <span class="md-ellipsis">
      Lab 10 - Monitoring
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Lab 10 - Monitoring">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-dashboards" class="md-nav__link">
    <span class="md-ellipsis">
      1. Dashboards
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-alerts" class="md-nav__link">
    <span class="md-ellipsis">
      2. Alerts
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-metrics" class="md-nav__link">
    <span class="md-ellipsis">
      3. Metrics
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lab-11-cicd-with-pipelines-tekton-and-argocd" class="md-nav__link">
    <span class="md-ellipsis">
      Lab 11 - CI/CD with Pipelines Tekton and ArgoCD
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Lab 11 - CI/CD with Pipelines Tekton and ArgoCD">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#continuous-integration-ci" class="md-nav__link">
    <span class="md-ellipsis">
      Continuous Integration (CI)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#continuous-delivery-cd" class="md-nav__link">
    <span class="md-ellipsis">
      Continuous Delivery (CD)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#deploy-an-application-using-ci-pipelines-with-tekton" class="md-nav__link">
    <span class="md-ellipsis">
      Deploy an Application using CI Pipelines with Tekton
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#promote-an-application-using-cd-with-gitops-gitea-config-repo-and-argocd" class="md-nav__link">
    <span class="md-ellipsis">
      Promote an Application using CD with GitOps (gitea config repo) and ArgoCD
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lab-12-application-modernization-with-transformation-advisor" class="md-nav__link">
    <span class="md-ellipsis">
      Lab 12 - Application modernization with Transformation Advisor
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lab-13-scale-your-application-with-gitops-and-argocd" class="md-nav__link">
    <span class="md-ellipsis">
      Lab 13 - Scale your application with GitOps and ArgoCD
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


<p><img alt="1" src="images/csm-redhat-wide.png" style="max-height:200px" /></p>
<h1 id="application-modernization-on-red-hat-openshift-technical-workshop">Application Modernization on Red Hat OpenShift - Technical workshop</h1>
<div class="admonition danger">
<p class="admonition-title">Disclaimer</p>
<p>This material has been created for training and learning purposes. It is not, by any means, official documentation supported by either IBM or Red Hat. </p>
</div>
<p><img alt="labArch" src="images/app-mod-ocp/labArch.png" style="max-height:600px" /></p>
<div class="admonition example">
<p class="admonition-title">Workshop environment</p>
<p>The environment for this workshop is composed of:</p>
<ul>
<li><strong>OpenShift cluster</strong></li>
<li><strong>RHEL bastion</strong></li>
</ul>
<p>Both are <strong>shared amongst all participants</strong>. The RHEL bastion is pre-configured with the <strong>scripts</strong>, <strong>assets</strong> and <strong>CLIs</strong> you will need to complete this workshop. All these will be placed under <strong>your user home directory</strong>.</p>
<p>The OpenShift cluster comes with an <strong>embedded terminal</strong> you can use to SSH into the RHEL bastion.</p>
<p><img alt="lab" src="images/app-mod-ocp/lab.png" style="max-height:800px" /></p>
<p>A <strong>project</strong> for each user has been created in the OpenShift cluster for them to work within.</p>
<p>You will also be provided with:</p>
<ul>
<li>
<p>The OpenShift cluser <strong>web console URL</strong></p>
<p><a href="https://console-openshift-console.apps.XXXXXX.cloud.techzone.ibm.com/">https://console-openshift-console.apps.XXXXXX.cloud.techzone.ibm.com/</a></p>
</li>
<li>
<p><strong>Username</strong> and <strong>password</strong> for the OpenShift cluster and the RHEL bastion.</p>
</li>
</ul>
</div>
<h2 id="lab-1-deploy-and-expose-an-application-using-red-hat-openshift-console">Lab 1 - Deploy and expose an application using Red Hat OpenShift console.</h2>
<h3 id="1-login-to-the-console">1. Login to the console</h3>
<!-- You can access via Menu -> OpenShift -> Clusters

![Arch](images/app-mod-ocp/1.1.01.png){: style="max-height:700px"}

Select the cluster pot-openshift and click on.
![Arch](images/app-mod-ocp/1.1.02.png){: style="max-height:700px"}

Then click on OpenShift Web Console button.
![Arch](images/app-mod-ocp/1.1.03.png){: style="max-height:700px"}

Alternative way: On the cluster list, on the ellipsis select OpenShift Web console.
![Arch](images/app-mod-ocp/1.1.04.png){: style="max-height:700px"}

Once you know the URL, you can access directly via browser. 
For example:

*https://console-openshift-console.pot-openshift-XXXXXXXXXXXXXXX-0000.eu-de.containers.appdomain.cloud/dashboards* -->

<p>Log into the OpenShift web console using the URL and credentials provided by the instructors.</p>
<p><img alt="1.1.05" src="images/app-mod-ocp/1.1.05.png" style="max-height:700px" /></p>
<p>OpenShift offers two perspectives: <strong>Administrator</strong> and <strong>Developer</strong>. You can use both if you have permissions. </p>
<p><img alt="1.1.06" src="images/app-mod-ocp/1.1.06.png" style="max-height:200px" /></p>
<p>If you click on <strong>Home</strong> --&gt; <strong>Projects</strong>, the page will show a list of the projects due you have reader permissions. You have not permissions to create a new one. You have just permissions to work with one of them. (Ask to the instructor which one is yours). </p>
<p><img alt="1.1.07" src="images/app-mod-ocp/1.1.07.png" style="max-height:700px" /></p>
<p>As you have rights to create Projects, you see the <strong>Create Project</strong> button.</p>
<p><img alt="1.1.08" src="images/app-mod-ocp/1.1.08.png" style="max-height:250px" /></p>
<p>On the Administrator perspective, <strong>Home</strong> --&gt; <strong>Overview</strong>, it will show a general dashboard with the health and status of the whole cluster. Something like this:</p>
<p><img alt="1.1.09" src="images/app-mod-ocp/1.1.09.png" style="max-height:700px" /></p>
<h3 id="2-deploy-a-new-application">2. Deploy a new application</h3>
<p>First, we are going to group our artifacts on a project (namespace).  Administrator has created each namespace already and gave the permissions for each user. </p>
<p>We are going to deploy a WebSphere Liberty official image. </p>
<p>Select the <strong>Developer</strong> perspective from left panel:</p>
<p><img alt="1.2.01" src="images/app-mod-ocp/1.2.01.png" style="max-height:300px" /></p>
<p>Click <strong>+Add</strong></p>
<p><img alt="1.2.02" src="images/app-mod-ocp/1.2.02.png" style="max-height:200px" /></p>
<p>Change to the project where you have permissions and click <strong>Container Image</strong>, for example if you are user <strong>user30</strong> select project <strong>ns30</strong> in the <strong>Project</strong> dropdown menu:</p>
<p><img alt="1.2.03" src="images/app-mod-ocp/1.2.03.png" style="max-height:900px" /></p>
<p>Enter the following information:</p>
<ol>
<li>Image<ul>
<li>Image Name: <strong>websphere-liberty:19.0.0.9-webProfile8-java11</strong></li>
</ul>
</li>
<li>General<ul>
<li>Application Name: <strong>websphere-liberty-app</strong></li>
</ul>
</li>
<li>Advanced Options<ul>
<li>Create a route to the application: <strong>checked</strong></li>
</ul>
</li>
<li>Click <strong>Create</strong> button.</li>
</ol>
<p><img alt="1.2.05" src="images/app-mod-ocp/1.2.05.png" style="max-height:1200px" /></p>
<p>If everything goes well, you should see the <strong>Topology</strong> view and the application deploying...</p>
<p>Press the websphere-liberty-app <strong>icon</strong> from the middle of the screen to see the status:</p>
<p><img alt="1.2.06" src="images/app-mod-ocp/1.2.06.png" style="max-height:800px" /></p>
<p>As you can see, a route has been automatically created for the application. It should be something like:</p>
<p><a href="http://websphere-liberty-app-nsX.apps.XXXXXX.cloud.techzone.ibm.com">http://websphere-liberty-app-nsX.apps.XXXXXX.cloud.techzone.ibm.com</a> </p>
<p><img alt="1.2.07" src="images/app-mod-ocp/1.2.07.png" style="max-height:700px" /></p>
<p>Now, from the same view we were before we can click on the <strong>pod</strong> link to check it's details:</p>
<p><img alt="1.2.08" src="images/app-mod-ocp/1.2.08.png" style="max-height:800px" /></p>
<p>Pod <strong>Details</strong></p>
<p><img alt="1.2.08-1" src="images/app-mod-ocp/1.2.08-1.png" style="max-height:1100px" /></p>
<p>It takes few seconds to get info on the pod <strong>Metrics</strong> graphs shown.</p>
<p><img alt="1.2.09" src="images/app-mod-ocp/1.2.09.png" style="max-height:1100px" /></p>
<p>Click <strong>Events</strong> tab. This is the first place to look for errors in runtime or during deployment:</p>
<p><img alt="1.2.10" src="images/app-mod-ocp/1.2.10.png" style="max-height:850px" /></p>
<p>And <strong>Logs</strong> tab:</p>
<p><img alt="1.2.11" src="images/app-mod-ocp/1.2.11.png" style="max-height:600px" /></p>
<p><strong>Terminal</strong> tab is also useful sometimes because you get a console inside the container itself:</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>This terminal is attached to a container within the pod you are at the moment so that you can run commands in that container. The workshop terminal which you use to ssh to the RHEL bastion is also a terminal attached to a container of a pod. A container that is pre-configured with certain tools (like the OpenShift CLI, ssh, etc) and integrated with the OpenShift Web Console. Don't confuse these.</p>
</div>
<p><img alt="1.2.12" src="images/app-mod-ocp/1.2.12.png" style="max-height:800px" /></p>
<p>In next section we will do the same but from command line understanding all the concepts that happened automatically under the covers when we used the OpenShift web console.</p>
<p>You can see the Service created using the <strong>Administration</strong> perspective:</p>
<p><img alt="1.2.13" src="images/app-mod-ocp/1.2.13.png" style="max-height:700px" /></p>
<p>Also, you can see the <strong>Route</strong> created: </p>
<p><img alt="1.2.14" src="images/app-mod-ocp/1.2.14.png" style="max-height:700px" /></p>
<p>And the <strong>ImageStream</strong>:</p>
<p><img alt="1.2.15" src="images/app-mod-ocp/1.2.15.png" style="max-height:700px" /></p>
<h2 id="lab-2-deploy-and-expose-an-application-using-red-hat-openshift-command-line">Lab 2 - Deploy and expose an application using Red Hat OpenShift command line</h2>
<h3 id="1-login-with-the-command-line">1. Login with the command line</h3>
<p>OpenShift can be configured using different Identity providers (LDAP, HttpPassword ...). But normally, you will login using a token.</p>
<p>To get this token, click on the dropdown user menu in the upper right corner of the page and click <strong>Copy Login Command</strong>:</p>
<p><img alt="2.1.01" src="images/app-mod-ocp/2.1.01.png" style="max-height:800px" /></p>
<p>On next page, click the <strong>Display Token</strong> link and copy the <strong>Log in with this token</strong> command line:</p>
<p><img alt="2.1.02" src="images/app-mod-ocp/2.1.02.png" style="max-height:450px" /></p>
<p>Paste the copied command in your terminal (where oc CLI is installed). </p>
<div class="admonition warning">
<p class="admonition-title">Important</p>
<p>To carry out the workshop, a RHEL bastion has been created where users can find the scripts, assets and CLIs required to perform the exercises. Before starting the workshop, all users should have received clear instructions as to how to SSH into that RHEL bastion and how to use it. Otherwise, ask the instructiors.</p>
</div>
<div class="no-copy highlight"><span class="filename">Output</span><pre><span></span><code>Logged into &quot;https://api.XXXXXX.cloud.techzone.ibm.com:6443&quot; as &quot;user30&quot; using the token provided.

You have access to 79 projects, the list has been suppressed. You can list all projects with &#39;oc projects&#39;

Using project &quot;default&quot;.
</code></pre></div>
<p>If login is successful, command result gives the number of projects you have permissions to, and which one is the default. As you have reader rights, you'll see all of them, but you just have writting permission on yours.</p>
<p>You can set your project as the default via: </p>
<div class="copy highlight"><span class="filename">Command</span><pre><span></span><code>oc project nsX
</code></pre></div>
<div class="no-copy highlight"><span class="filename">Output</span><pre><span></span><code>Now using project &quot;ns30&quot; on server &quot;https://api.XXXXX.cloud.techzone.ibm.com:6443&quot;.
</code></pre></div>
<p>We have installed the OpenShift command line for you in the RHEL bastion. However, it you wanted to also install the command line in your own desktop, click on help icon, select <strong>Command Line Tools</strong> and follow the instructions from the OpenShift console:</p>
<p><img alt="2.1.04" src="images/app-mod-ocp/2.1.04.png" style="max-height:700px" /></p>
<h3 id="2-deploy-a-new-application_1">2. Deploy a new application</h3>
<div class="admonition info">
<p class="admonition-title">Reference</p>
<p><a href="https://docs.openshift.com/container-platform/4.12/applications/deployments/what-deployments-are.html">https://docs.openshift.com/container-platform/4.12/applications/deployments/what-deployments-are.html</a></p>
</div>
<p>Now we can execute commands against the cluster.</p>
<p>First, let's make sure we clean all existing artifacts from previous lab. To remove the deployment, on <strong>Administrator</strong> go to <strong>Workloads</strong> --&gt; <strong>Deployments</strong>. On the <strong>websphere-liberty-app</strong> right side three-dots options drop down menu, select <strong>Delete Deployment</strong>.</p>
<p><img alt="2.2.01" src="images/app-mod-ocp/2.2.01.png" style="max-height:700px" /></p>
<p>Select <strong>Delete</strong> when prompted for confirmation making sure you check the two boxes available so that not only the deployment is deleted but also the service and the route.</p>
<p><img alt="2.2.02" src="images/app-mod-ocp/2.2.02.png" style="max-height:300px" /></p>
<!-- !!! danger "Important"
    The above check doesn't delete the **Route**, **Service** and **ImageStream**. Go to each of those sections and delete them manually.

Example with **Networking** -> **Services**

![2.2.03](images/app-mod-ocp/2.2.03.png){: style="max-height:700px"}

Verify and repeat same procedure with:

* **Networking** -> **Routes**
* **Builds** -> **Image Streams** -->

<p>Once verified that the resources have been deleted, let's define our first OpenShift deployment. Create a file called <strong>was-deployment.yaml</strong> with the following content:</p>
<div class="copy highlight"><pre><span></span><code><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">apps/v1</span>
<span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Deployment</span>
<span class="nt">metadata</span><span class="p">:</span>
<span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">websphere-liberty-app</span>
<span class="hll"><span class="w">  </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">nsX</span>
</span><span class="nt">spec</span><span class="p">:</span>
<span class="w">  </span><span class="nt">selector</span><span class="p">:</span>
<span class="w">    </span><span class="nt">matchLabels</span><span class="p">:</span>
<span class="w">      </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">websphere-liberty-app</span>
<span class="w">  </span><span class="nt">replicas</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span>
<span class="w">  </span><span class="nt">template</span><span class="p">:</span>
<span class="w">    </span><span class="nt">metadata</span><span class="p">:</span>
<span class="w">      </span><span class="nt">labels</span><span class="p">:</span>
<span class="w">        </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">websphere-liberty-app</span>
<span class="w">    </span><span class="nt">spec</span><span class="p">:</span>
<span class="w">      </span><span class="nt">containers</span><span class="p">:</span>
<span class="w">        </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">websphere-liberty-app</span>
<span class="w">          </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">websphere-liberty:19.0.0.9-webProfile8-java11</span>
<span class="w">          </span><span class="nt">ports</span><span class="p">:</span>
<span class="w">            </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">containerPort</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">9080</span>
</code></pre></div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>The <strong>indentation</strong> is very important. Pay attention when executing the copy and paste. If you have problems with the copy and paste, <strong>all the files used in the labs are in the artifact folder under the user home folder</strong>.</p>
</div>
<p>Because the file exists in <code>/artifacts</code> directory, execute:</p>
<div class="copy highlight"><span class="filename">Command</span><pre><span></span><code><span class="nb">cd</span><span class="w"> </span>/home/userX/artifacts<span class="w"> </span>
oc<span class="w"> </span>create<span class="w"> </span>-f<span class="w"> </span>was-deployment.yaml
</code></pre></div>
<p>If everything went well, your pod should be running. Check it out executing:</p>
<div class="copy highlight"><span class="filename">Command</span><pre><span></span><code>oc<span class="w"> </span>get<span class="w"> </span>pods<span class="w"> </span>nsX
</code></pre></div>
<div class="no-copy highlight"><span class="filename">Output</span><pre><span></span><code>NAME                                     READY   STATUS    RESTARTS   AGE
websphere-liberty-app-6fb8f5b9fd-x4c89   1/1     Running   0          37s
</code></pre></div>
<p>Pod could be on ContainerCreating Status. Let's wait few seconds and try again. 
Application is running but it can't be accessed. </p>
<p>We need to define a service to expose it internally or externally. Let's see how in next section.
 </p>
<h3 id="3-expose-the-application">3. Expose the application</h3>
<div class="admonition info">
<p class="admonition-title">Reference</p>
<p><a href="https://docs.openshift.com/container-platform/4.12/networking/understanding-networking.html">https://docs.openshift.com/container-platform/4.12/networking/understanding-networking.html</a></p>
</div>
<h4 id="31-cluster-ip">3.1  Cluster IP</h4>
<p>Let's start exposing the application just internally via <strong>ClusterIP</strong> service type. As we did before, create a file called <strong>was-service1.yaml</strong> with this content:</p>
<div class="copy highlight"><pre><span></span><code><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">v1</span>
<span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Service</span>
<span class="nt">metadata</span><span class="p">:</span>
<span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">websphere-liberty-service1</span>
<span class="hll"><span class="w">  </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">nsX</span>
</span><span class="nt">spec</span><span class="p">:</span>
<span class="w">  </span><span class="nt">selector</span><span class="p">:</span>
<span class="w">    </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">websphere-liberty-app</span>
<span class="w">  </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ClusterIP</span>
<span class="w">  </span><span class="nt">ports</span><span class="p">:</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">protocol</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">TCP</span>
<span class="w">      </span><span class="nt">port</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">9080</span>
<span class="w">      </span><span class="nt">targetPort</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">9080</span>
</code></pre></div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>The <strong>indentation</strong> is very important. Pay attention when executing the copy and paste. If you have problems with the copy and paste, <strong>all the files used in the labs are in the artifact folder under the user home folder</strong>.</p>
</div>
<p>Once you have the file created, execute:</p>
<div class="copy highlight"><span class="filename">Command</span><pre><span></span><code>oc<span class="w"> </span>create<span class="w"> </span>-f<span class="w"> </span>was-service1.yaml
</code></pre></div>
<div class="no-copy highlight"><span class="filename">Output</span><pre><span></span><code>sevice/websphere-liberty-service1 created
</code></pre></div>
<p>If everything went well, your service should be deployed, and the application accessed from inside the cluster (this means other pods running in it):</p>
<div class="copy highlight"><span class="filename">Command</span><pre><span></span><code>oc<span class="w"> </span>get<span class="w"> </span>services<span class="w"> </span>-n<span class="w"> </span>nsX
</code></pre></div>
<div class="no-copy highlight"><span class="filename">Output</span><pre><span></span><code>NAME                         TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)    AGE
websphere-liberty-service1   ClusterIP   172.30.82.44   &lt;none&gt;        9080/TCP   10s
</code></pre></div>
<div class="admonition warning">
<p class="admonition-title">Note</p>
<p>If you don't find the service, double check your default project (<code>oc projects</code> or <code>oc get services --all-namespaces</code>).</p>
</div>
<p>Executing the following command, you can also see the mapping to the real endpoints (your application pods). It doesn't matter if your application scales up, scales down, moves to other worker... that will be managed internally by the service. You just access to the service name or its internal IP:</p>
<div class="copy highlight"><span class="filename">Command</span><pre><span></span><code>oc<span class="w"> </span>describe<span class="w"> </span>service<span class="w"> </span>websphere-liberty-service1<span class="w"> </span>-n<span class="w"> </span>nsX
</code></pre></div>
<div class="no-copy highlight"><span class="filename">Output</span><pre><span></span><code><span class="nt">Name</span><span class="p">:</span><span class="w">              </span><span class="l l-Scalar l-Scalar-Plain">websphere-liberty-service1</span>
<span class="nt">Namespace</span><span class="p">:</span><span class="w">         </span><span class="l l-Scalar l-Scalar-Plain">ns30</span>
<span class="nt">Labels</span><span class="p">:</span><span class="w">            </span><span class="l l-Scalar l-Scalar-Plain">&lt;none&gt;</span>
<span class="nt">Annotations</span><span class="p">:</span><span class="w">       </span><span class="l l-Scalar l-Scalar-Plain">&lt;none&gt;</span>
<span class="nt">Selector</span><span class="p">:</span><span class="w">          </span><span class="l l-Scalar l-Scalar-Plain">app=websphere-liberty-app</span>
<span class="nt">Type</span><span class="p">:</span><span class="w">              </span><span class="l l-Scalar l-Scalar-Plain">ClusterIP</span>
<span class="nt">IP Family Policy</span><span class="p">:</span><span class="w">  </span><span class="l l-Scalar l-Scalar-Plain">SingleStack</span>
<span class="nt">IP Families</span><span class="p">:</span><span class="w">       </span><span class="l l-Scalar l-Scalar-Plain">IPv4</span>
<span class="nt">IP</span><span class="p">:</span><span class="w">                </span><span class="l l-Scalar l-Scalar-Plain">172.30.82.44</span>
<span class="nt">IPs</span><span class="p">:</span><span class="w">               </span><span class="l l-Scalar l-Scalar-Plain">172.30.82.44</span>
<span class="nt">Port</span><span class="p">:</span><span class="w">              </span><span class="l l-Scalar l-Scalar-Plain">&lt;unset&gt;  9080/TCP</span>
<span class="nt">TargetPort</span><span class="p">:</span><span class="w">        </span><span class="l l-Scalar l-Scalar-Plain">9080/TCP</span>
<span class="hll"><span class="nt">Endpoints</span><span class="p">:</span><span class="w">         </span><span class="l l-Scalar l-Scalar-Plain">10.131.0.87:9080</span>
</span><span class="nt">Session Affinity</span><span class="p">:</span><span class="w">  </span><span class="l l-Scalar l-Scalar-Plain">None</span>
<span class="nt">Events</span><span class="p">:</span><span class="w">            </span><span class="l l-Scalar l-Scalar-Plain">&lt;none&gt;</span>
</code></pre></div>
<p>If <strong>Endpoints</strong> is empty, it means the application has not been found. Review the steps on the previous chapter.</p>
<p>We can check the IP of our pod, to see that it matches the endpoint from the service, with the following command:</p>
<div class="copy highlight"><span class="filename">Command</span><pre><span></span><code>oc<span class="w"> </span>get<span class="w"> </span>pods<span class="w"> </span>-o<span class="w"> </span>wide<span class="w"> </span>-n<span class="w"> </span>nsX
</code></pre></div>
<div class="no-copy highlight"><span class="filename">Output</span><pre><span></span><code>NAME                                     READY   STATUS    RESTARTS   AGE     IP            NODE       NOMINATED NODE   READINESS GATES
websphere-liberty-app-6fb8f5b9fd-x4c89   1/1     Running   0          4m12s   10.131.0.87   worker-2   &lt;none&gt;           &lt;none&gt;
</code></pre></div>
<p>But as we said, this type of service is accessible only from inside the cluster itself. We have other two types of services to expose the application externally.</p>
<!-- 
#### 3.2 NodePort

!!! info "Reference"
    https://kubernetes.io/docs/concepts/services-networking/service/#nodeport

Create a file called **was-service2.yaml** with this content:

<div class="copy highlight"><pre><span></span><code><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">v1</span>
<span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Service</span>
<span class="nt">metadata</span><span class="p">:</span>
<span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">websphere-liberty-service2</span>
<span class="hll"><span class="w">  </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">nsX</span>
</span><span class="nt">spec</span><span class="p">:</span>
<span class="w">  </span><span class="nt">selector</span><span class="p">:</span>
<span class="w">    </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">websphere-liberty-app</span>
<span class="w">  </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">NodePort</span>
<span class="w">  </span><span class="nt">ports</span><span class="p">:</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">protocol</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">TCP</span>
<span class="w">      </span><span class="nt">port</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">9080</span>
<span class="w">      </span><span class="nt">targetPort</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">9080</span>
</code></pre></div>

!!! tip
    The **indentation** is very important. Pay attention when executing the copy and paste. If you have problems with the copy and paste, **all the files used in the labs are in the artifact folder under the user home folder**.

Once you have the file created, execute:

<div class="copy highlight"><span class="filename">Command</span><pre><span></span><code>oc<span class="w"> </span>create<span class="w"> </span>-f<span class="w"> </span>was-service2.yaml
</code></pre></div>

<div class="no-copy highlight"><span class="filename">Output</span><pre><span></span><code>sevice/websphere-liberty-service2 created
</code></pre></div>

If everything went well, your service should be deployed:

<div class="copy highlight"><span class="filename">Command</span><pre><span></span><code>oc<span class="w"> </span>get<span class="w"> </span>services<span class="w"> </span>-n<span class="w"> </span>nsX
</code></pre></div>

<div class="no-copy highlight"><span class="filename">Output</span><pre><span></span><code>NAME                         TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE
websphere-liberty-service1   ClusterIP   172.30.82.44     &lt;none&gt;        9080/TCP         5m2s
websphere-liberty-service2   NodePort    172.30.206.129   &lt;none&gt;        9080:32526/TCP   11s
</code></pre></div>

A service of type NodePort is a ClusterIP service plus a NodePort. With NodePort, OpenShift enables on all the nodes that specific port to access the application from outside the cluster. This port is a random number between 30000 and 32767. It can be fixed in the service yaml file (but you should be sure it is not already taken).

So now, we can access to our application browsing directly to any node IP plus the assigned NodePort. 

!!! warning "Note"
    On this workshop environment, you dont have permissions to see node related information via command line (`oc get nodes`).

!!! danger
    **REVIEW** **REVIEW** **REVIEW** **REVIEW** **REVIEW**

![Arch](images/app-mod-ocp/2.2.06.png){: style="max-height:700px"}

But you can access the public IPs via the cluster itself. 

Menu on IBM Cloud -> OpenShift -> Clusters

![Arch](images/app-mod-ocp/2.2.07.png){: style="max-height:700px"}

Select the cluster:

![Arch](images/app-mod-ocp/2.2.08.png){: style="max-height:700px"}

Go to Worker nodes section and see the Public IP address:

![Arch](images/app-mod-ocp/2.2.09.png){: style="max-height:700px"}

In our case, change the IP with one shown before, and the port with your Node Port.  http://158.xxx.xxx.xxx:31406/ (use your NodePort),

![Arch](images/app-mod-ocp/2.2.10.png){: style="max-height:700px"}

NodePort is nice, we get TCP access for example but without features like HTTP/HTTPS, session affinity, etc... In the next section we will see how to get access to that kind of features.

!!! danger
    **REVIEW** **REVIEW** **REVIEW** **REVIEW** **REVIEW** -->

<h4 id="33-ingress">3.3 Ingress</h4>
<div class="admonition info">
<p class="admonition-title">Reference</p>
<p><a href="https://docs.openshift.com/container-platform/4.12/networking/ingress-operator.html">https://docs.openshift.com/container-platform/4.12/networking/ingress-operator.html</a></p>
</div>
<p>Once we have a service, we can create an Ingress object. An Ingress object is used to expose an application to consumers outside the OpenShift cluster. It uses a OpenShift service (ClusterIP) to know the internal IPs used by the application. </p>
<p>The Ingress Controller used by default in OpenShift is an <strong><a href="https://github.com/openshift/cluster-ingress-operator">HAProxy</a></strong>, but there are more ingress controllers available (i.e. F5 Ingress Controller).</p>
<p>Create a file called <strong>was-ingress.yaml</strong> with this content:</p>
<div class="copy highlight"><pre><span></span><code><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">networking.k8s.io/v1</span>
<span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Ingress</span>
<span class="nt">metadata</span><span class="p">:</span>
<span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">websphere-liberty-ingress</span>
<span class="hll"><span class="w">  </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">nsX</span>
</span><span class="nt">spec</span><span class="p">:</span>
<span class="w">  </span><span class="nt">rules</span><span class="p">:</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">host</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">websphere-liberty-app-nsX.apps.XXXXXX.cloud.techzone.ibm.com</span>
<span class="w">      </span><span class="nt">http</span><span class="p">:</span>
<span class="w">        </span><span class="nt">paths</span><span class="p">:</span>
<span class="w">          </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">path</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">/</span>
<span class="w">            </span><span class="nt">pathType</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Exact</span>
<span class="w">            </span><span class="nt">backend</span><span class="p">:</span>
<span class="w">              </span><span class="nt">service</span><span class="p">:</span>
<span class="w">                </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">websphere-liberty-service1</span>
<span class="w">                </span><span class="nt">port</span><span class="p">:</span><span class="w"> </span>
<span class="w">                  </span><span class="nt">number</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">9080</span>
</code></pre></div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>The <strong>indentation</strong> is very important. Pay attention when executing the copy and paste. If you have problems with the copy and paste, <strong>all the files used in the labs are in the artifact folder under the user home folder</strong>.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Important</p>
<p>The ingress subdomain XXXXXX must be the one assigned to the cluster. In this case: <strong>apps.XXXXXX.cloud.techzone.ibm.com</strong></p>
<p>You can get it from the very same OpenShift web console URL you have your browser pointing to.</p>
</div>
<!-- You can get it from:
Menu on IBM Cloud -> OpenShift -> Clusters -> Select your cluster -> Overview
![Arch](images/app-mod-ocp/2.2.11.png){: style="max-height:700px"} -->

<p>Once you have the file created, execute:</p>
<div class="copy highlight"><span class="filename">Command</span><pre><span></span><code>oc<span class="w"> </span>create<span class="w"> </span>-f<span class="w"> </span>was-ingress.yaml<span class="w"> </span>-n<span class="w"> </span>nsX
</code></pre></div>
<div class="no-copy highlight"><span class="filename">Output</span><pre><span></span><code>ingress.networking.k8s.io/websphere-liberty-ingress created
</code></pre></div>
<p>If everything went well, your service should be deployed:</p>
<div class="copy highlight"><span class="filename">Command</span><pre><span></span><code>oc<span class="w"> </span>get<span class="w"> </span>ingress<span class="w"> </span>-n<span class="w"> </span>nsX
</code></pre></div>
<div class="no-copy highlight"><span class="filename">Output</span><pre><span></span><code>NAME                        CLASS    HOSTS                                                                             ADDRESS                                                               PORTS   AGE
websphere-liberty-ingress   &lt;none&gt;   websphere-liberty-app-ns30.apps.65f05030777e25001ee8fdfa.cloud.techzone.ibm.com   router-default.apps.65f05030777e25001ee8fdfa.cloud.techzone.ibm.com   80      17s
</code></pre></div>
<p>With this Ingress object, OpenShift has configured the ingress controller to route traffic from internal routers to the application/deployment in the cluster.</p>
<p>The "rule" is that all the requests made for the hostname</p>
<p><a href="http://websphere-liberty-app-nsX.apps.XXXXXX.cloud.techzone.ibm.com">http://websphere-liberty-app-nsX.apps.XXXXXX.cloud.techzone.ibm.com</a></p>
<p>must be routed directly to the pods selected by the service <strong>websphere-liberty-service1</strong>. This enables, for example, sticky sessions. There are other options to configure the rules not only by hostname. For example, it is also possible to use the path.</p>
<p>Now you can access the URL</p>
<p><a href="http://websphere-liberty-app-nsX.apps.XXXXXX.cloud.techzone.ibm.com">http://websphere-liberty-app-nsX.apps.XXXXXX.cloud.techzone.ibm.com</a></p>
<p>in order to access your application. As we are using the cluster's ingress subdomain you can access directly.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Remember, it is exposed over <strong>http</strong> rather than https.</p>
</div>
<p><img alt="2.2.13" src="images/app-mod-ocp/2.2.13.png" style="max-height:700px" /></p>
<p>You can also try with curl:</p>
<p><div class="copy highlight"><span class="filename">Command</span><pre><span></span><code>curl<span class="w"> </span>-s<span class="w"> </span>http://websphere-liberty-app-nsX.apps.XXXXXX.cloud.techzone.ibm.com/<span class="w"> </span><span class="p">|</span><span class="w"> </span>grep<span class="w"> </span>WebSphere
</code></pre></div>
<div class="no-copy highlight"><span class="filename">Output</span><pre><span></span><code><span class="p">&lt;</span><span class="nt">title</span><span class="p">&gt;</span>WebSphere Liberty 19.0.0.9<span class="p">&lt;/</span><span class="nt">title</span><span class="p">&gt;</span>
            <span class="p">&lt;</span><span class="nt">h2</span><span class="p">&gt;</span>WebSphere Liberty 19.0.0.9<span class="p">&lt;/</span><span class="nt">h2</span><span class="p">&gt;</span>
</code></pre></div></p>
<h3 id="4-clean-all-the-artifacts">4. Clean all the artifacts</h3>
<p>To delete all artifacts, you can use the previous files that define the object:</p>
<div class="copy highlight"><span class="filename">Command</span><pre><span></span><code>oc<span class="w"> </span>delete<span class="w"> </span>-f<span class="w"> </span>was-service1.yaml
oc<span class="w"> </span>delete<span class="w"> </span>-f<span class="w"> </span>was-service2.yaml
oc<span class="w"> </span>delete<span class="w"> </span>-f<span class="w"> </span>was-ingress.yaml
oc<span class="w"> </span>delete<span class="w"> </span>-f<span class="w"> </span>was-deployment.yaml
</code></pre></div>
<h2 id="lab-3-storage">Lab 3 - Storage</h2>
<h3 id="1-check-your-current-application">1. Check your current application</h3>
<p>First, we are going to check what happens when you have not assigned external persistent storage on your application.</p>
<p>Create your application:</p>
<div class="copy highlight"><span class="filename">Command</span><pre><span></span><code>oc<span class="w"> </span>create<span class="w"> </span>-f<span class="w"> </span>was-deployment.yaml
</code></pre></div>
<p>Create a file internally in the pod via the pod's <strong>Terminal</strong>:</p>
<p><img alt="3.1.01" src="images/app-mod-ocp/3.1.01.png" style="max-height:400px" /></p>
<p>Inside the <strong>Terminal</strong>, go to logs folder and create a file </p>
<div class="copy highlight"><span class="filename">Command</span><pre><span></span><code><span class="nb">cd</span><span class="w"> </span>logs
touch<span class="w"> </span>hola.txt
</code></pre></div>
<p><img alt="3.1.02" src="images/app-mod-ocp/3.1.02.png" style="max-height:500px" /></p>
<p>Delete the existing pod to create a new one. OpenShift will create it automatically again.</p>
<p><img alt="3.1.03" src="images/app-mod-ocp/3.1.03.png" style="max-height:500px" /></p>
<p>Go to the new pod and check with the <strong>Terminal</strong> the <code>/logs</code> folder again. You will observe that <code>hola.txt</code> file doesn't exist.</p>
<p>This is because containers are ephemeral, and we have not used external persistence.</p>
<p><img alt="3.1.04" src="images/app-mod-ocp/3.1.04.png" style="max-height:450px" /></p>
<h3 id="2-storage-classes">2. Storage Classes</h3>
<div class="admonition info">
<p class="admonition-title">Reference</p>
<ul>
<li><a href="https://docs.openshift.com/container-platform/4.12/storage/understanding-persistent-storage.html#pvc-storage-class_understanding-persistent-storage">https://docs.openshift.com/container-platform/4.12/storage/understanding-persistent-storage.html#pvc-storage-class_understanding-persistent-storage</a></li>
<li><a href="https://docs.openshift.com/container-platform/4.12/storage/dynamic-provisioning.html">https://docs.openshift.com/container-platform/4.12/storage/dynamic-provisioning.html</a></li>
</ul>
</div>
<p>As we have commented already, <strong>Storage Classes</strong> are used to provision dynamic <strong>Persistent Volumes</strong>.</p>
<p>During this lab we will configure external storage in Red Hat OpenShift. This storage will allow to our application to use external persistence. It can be shared storage or not, depending on the storage technology we use.</p>
<p>There are several options for the storage, like: NFS, OpenShift Data Foundation (old OpenShift Container Storage), Spectrum Scale, Cloud providers... For simplicity, we will use the ODF storage class coming out of the box with the cluster for this workshop.</p>
<p>You can check the existing storage classes available on the cluster:</p>
<p><img alt="3.2.01" src="images/app-mod-ocp/3.2.01.png" style="max-height:650px" /></p>
<p>You can see them also via command line:</p>
<div class="copy highlight"><span class="filename">Command</span><pre><span></span><code>oc<span class="w"> </span>get<span class="w"> </span>storageclasses
</code></pre></div>
<div class="no-copy highlight"><span class="filename">Output</span><pre><span></span><code>NAME                          PROVISIONER                             RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE
localblock                    kubernetes.io/no-provisioner            Delete          WaitForFirstConsumer   false                  14d
ocs-storagecluster-ceph-rbd   openshift-storage.rbd.csi.ceph.com      Delete          Immediate              true                   14d
ocs-storagecluster-ceph-rgw   openshift-storage.ceph.rook.io/bucket   Delete          Immediate              false                  14d
ocs-storagecluster-cephfs     openshift-storage.cephfs.csi.ceph.com   Delete          Immediate              true                   14d
openshift-storage.noobaa.io   openshift-storage.noobaa.io/obc         Delete          Immediate              false                  14d
</code></pre></div>
<p>You can install other types such as NFS but we are going to use the default one.</p>
<p>Delete all artifacts:</p>
<div class="copy highlight"><span class="filename">Command</span><pre><span></span><code>oc<span class="w"> </span>delete<span class="w"> </span>-f<span class="w"> </span>was-deployment.yaml
</code></pre></div>
<h3 id="3-create-a-persistent-volume-claim">3. Create a Persistent Volume Claim</h3>
<p>We will use again our WebSphere Liberty sample application. We will externalize logs directory from the container to maintain existing logs after pod restart.</p>
<p>We are going to create a dynamic Persistence Volume via the Storage Class. Let's create a Persistent Volume Claim going to:</p>
<p><img alt="3.3.01" src="images/app-mod-ocp/3.3.01.png" style="max-height:650px" /></p>
<p>Change all the values marked below (notice the app Selector must match with your user). Please take care and just ask for 10 Mi.</p>
<p><img alt="3.3.02" src="images/app-mod-ocp/3.3.02.png" style="max-height:700px" /></p>
<p>Or copy the following content on the <strong>Edit YAML</strong>, updating the red marked texts:</p>
<div class="copy highlight"><pre><span></span><code><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">v1</span>
<span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">PersistentVolumeClaim</span>
<span class="nt">metadata</span><span class="p">:</span>
<span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">was-volume1</span>
<span class="hll"><span class="w">  </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">nsX</span>
</span><span class="nt">spec</span><span class="p">:</span>
<span class="w">  </span><span class="nt">accessModes</span><span class="p">:</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ReadWriteMany</span>
<span class="w">  </span><span class="nt">resources</span><span class="p">:</span>
<span class="w">    </span><span class="nt">requests</span><span class="p">:</span>
<span class="w">      </span><span class="nt">storage</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">10Mi</span>
<span class="w">  </span><span class="nt">storageClassName</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ocs-storagecluster-cephfs</span>
</code></pre></div>
<p>And press <strong>Create</strong> button.</p>
<p>If everything goes well, it should appear with <strong>Bound</strong> status.</p>
<p><img alt="3.3.03" src="images/app-mod-ocp/3.3.03.png" style="max-height:900px" /></p>
<div class="admonition info">
<p class="admonition-title">Info</p>
<p>The way we can link the PVC with an existing PV is via a selector. In the Persistent Volume Claim we can specify the PV label, for example:</p>
<div class="no-copy highlight"><pre><span></span><code><span class="nt">selector</span><span class="p">:</span>
<span class="w">  </span><span class="nt">matchLabels</span><span class="p">:</span>
<span class="hll"><span class="w">    </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">myPersistentVolume</span>
</span></code></pre></div>
<p>If you do not specify a label or a Storage Class, the bind is done randomly always considering the requirements of the PVC and the characteristics of existing PVs. If no matching PV is found it will remain in <strong>Pending</strong> status.</p>
</div>
<h3 id="4-use-the-persistence-volume-claim-in-an-application">4. Use the persistence volume claim in an application</h3>
<p>Let's deploy again a WebSphere Liberty image but this time mapping its <code>/logs</code> directory to the external persistence volume we have created. Everything should be familiar except some new lines related to the persistence.</p>
<p>You can create a file called <strong>was-deployment-pvc.yaml</strong> file with this content (or use the one on the artifacts folder created for you):</p>
<div class="copy highlight"><pre><span></span><code><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">apps/v1</span>
<span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Deployment</span>
<span class="nt">metadata</span><span class="p">:</span>
<span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">websphere-liberty-app</span>
<span class="hll"><span class="w">  </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">nsX</span>
</span><span class="nt">spec</span><span class="p">:</span>
<span class="w">  </span><span class="nt">selector</span><span class="p">:</span>
<span class="w">    </span><span class="nt">matchLabels</span><span class="p">:</span>
<span class="w">      </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">websphere-liberty-app</span>
<span class="w">  </span><span class="nt">replicas</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span>
<span class="w">  </span><span class="nt">template</span><span class="p">:</span>
<span class="w">    </span><span class="nt">metadata</span><span class="p">:</span>
<span class="w">      </span><span class="nt">labels</span><span class="p">:</span>
<span class="w">        </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">websphere-liberty-app</span>
<span class="w">    </span><span class="nt">spec</span><span class="p">:</span>
<span class="w">      </span><span class="nt">containers</span><span class="p">:</span>
<span class="w">        </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">websphere-liberty-app</span>
<span class="w">          </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">websphere-liberty:19.0.0.9-webProfile8-java11</span>
<span class="w">          </span><span class="nt">ports</span><span class="p">:</span>
<span class="w">            </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">containerPort</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">9080</span>
<span class="w">          </span><span class="nt">volumeMounts</span><span class="p">:</span>
<span class="w">          </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">mountPath</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;/logs&quot;</span>
<span class="w">            </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">was-persistence</span>
<span class="w">      </span><span class="nt">volumes</span><span class="p">:</span>
<span class="w">        </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">was-persistence</span>
<span class="w">          </span><span class="nt">persistentVolumeClaim</span><span class="p">:</span>
<span class="w">            </span><span class="nt">claimName</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">was-volume1</span>
<span class="w">      </span><span class="nt">initContainers</span><span class="p">:</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">permissionsfix</span>
<span class="w">        </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">alpine:latest</span>
<span class="w">        </span><span class="nt">command</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="s">&quot;/bin/sh&quot;</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">&quot;-c&quot;</span><span class="p p-Indicator">]</span>
<span class="w">        </span><span class="nt">args</span><span class="p">:</span>
<span class="w">          </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">chown 1001:0 /prueba;</span>
<span class="w">        </span><span class="nt">volumeMounts</span><span class="p">:</span>
<span class="w">        </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">was-persistence</span>
<span class="w">          </span><span class="nt">mountPath</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">/prueba</span>
</code></pre></div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>The <strong>indentation</strong> is very important. Pay attention when executing the copy and paste. If you have problems with the copy and paste, <strong>all the files used in the labs are in the artifact folder under the user home folder</strong>.</p>
</div>
<p>Once you have the file created, execute:</p>
<div class="copy highlight"><span class="filename">Command</span><pre><span></span><code>oc<span class="w"> </span>create<span class="w"> </span>-f<span class="w"> </span>was-deployment-pvc.yaml
</code></pre></div>
<div class="no-copy output highlight"><span class="filename">Output</span><pre><span></span><code>deployment.apps/websphere-liberty-app created
</code></pre></div>
<p>If everything went well, your pod should be running:</p>
<div class="copy highlight"><span class="filename">Command</span><pre><span></span><code>oc<span class="w"> </span>get<span class="w"> </span>pods<span class="w"> </span>-n<span class="w"> </span>nsX
</code></pre></div>
<div class="no-copy output highlight"><span class="filename">Output</span><pre><span></span><code>NAME                                     READY   STATUS    RESTARTS   AGE
websphere-liberty-app-658997c5cf-6fhp2   1/1     Running   0          55s
</code></pre></div>
<p>We are going to create a file again on the logs folder. Now we are going to do it via command line.</p>
<ol>
<li>
<p>Exec into the WebSphere Liberty app pod</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Replace <code>websphere-liberty-app-658997c5cf-6fhp2</code> with the output from the previous command and <code>nsX</code> with your user number.</p>
</div>
<div class="copy highlight"><span class="filename">Command</span><pre><span></span><code>oc<span class="w"> </span>rsh<span class="w"> </span>websphere-liberty-app-658997c5cf-6fhp2<span class="w"> </span>-n<span class="w"> </span>nsX
</code></pre></div>
</li>
<li>
<p>Create a new file
    <div class="copy highlight"><span class="filename">Command</span><pre><span></span><code><span class="nb">cd</span><span class="w"> </span>logs
touch<span class="w"> </span>hola.txt
ls<span class="w"> </span>-la
</code></pre></div></p>
<div class="no-copy output highlight"><span class="filename">Output</span><pre><span></span><code>total 6
drwxr-xr-x. 2 default root    2 Mar 27 14:49 .
dr-xr-xr-x. 1 root    root   50 Mar 27 14:47 ..
-rw-rw-rw-. 1 default root    0 Mar 27 14:49 hola.txt
-rw-r-----. 1 default root 5344 Mar 27 14:47 messages.log
</code></pre></div>
</li>
<li>
<p>Type "<strong>exit</strong>" to finish the pod shell session</p>
</li>
</ol>
<p>Now we are going to delete the pod via command line:</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Replace <code>websphere-liberty-app-658997c5cf-6fhp2</code> with the output from the previous command and <code>nsX</code> with your user number.</p>
</div>
<div class="copy highlight"><span class="filename">Command</span><pre><span></span><code>oc<span class="w"> </span>delete<span class="w"> </span>pod<span class="w"> </span>websphere-liberty-app-658997c5cf-6fhp2<span class="w"> </span>-n<span class="w"> </span>nsX
</code></pre></div>
<p>You can see the pod is deleted and another one is created automatically.</p>
<div class="no-copy output highlight"><span class="filename">Output</span><pre><span></span><code>pod &quot;websphere-liberty-app-658997c5cf-6fhp2&quot; deleted

oc get pods

NAME                                     READY   STATUS    RESTARTS   AGE
websphere-liberty-app-658997c5cf-g4lf7   1/1     Running   0          17s
</code></pre></div>
<p>Enter in the new pod shell and check <em>hola.txt</em> file continues existing.</p>
<ol>
<li>
<p>Exec into the WebSphere Liberty app pod</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Replace <code>websphere-liberty-app-658997c5cf-g4lf7</code> with the output from the previous command and <code>nsX</code> with your user number.</p>
</div>
<div class="copy highlight"><span class="filename">Command</span><pre><span></span><code>oc<span class="w"> </span>rsh<span class="w"> </span>websphere-liberty-app-658997c5cf-g4lf7<span class="w"> </span>-n<span class="w"> </span>nsX
</code></pre></div>
</li>
<li>
<p>Check the <em>hola.txt</em> file exists</p>
<div class="copy highlight"><span class="filename">Command</span><pre><span></span><code><span class="nb">cd</span><span class="w"> </span>logs
ls<span class="w"> </span>-la
</code></pre></div>
<div class="no-copy output highlight"><span class="filename">Output</span><pre><span></span><code>total 13
drwxr-xr-x. 2 default root    3 Mar 27 14:50 .
dr-xr-xr-x. 1 root    root   50 Mar 27 14:50 ..
-rw-rw-rw-. 1 default root    0 Mar 27 14:49 hola.txt
-rw-r-----. 1 default root 6793 Mar 27 14:50 messages_24.03.27_14.50.21.0.log
-rw-r-----. 1 default root 5344 Mar 27 14:50 messages.log
</code></pre></div>
</li>
<li>
<p>Type "<strong>exit</strong>" to finish the pod shell session</p>
</li>
</ol>
<h3 id="5-clean-all-the-artifacts">5. Clean all the artifacts</h3>
<p>To delete all artifacts, you can use the previous files:</p>
<div class="copy highlight"><span class="filename">Command</span><pre><span></span><code>oc<span class="w"> </span>delete<span class="w"> </span>-f<span class="w"> </span>was-deployment-pvc.yaml
oc<span class="w"> </span>delete<span class="w"> </span>-f<span class="w"> </span>was-pvc1.yaml
</code></pre></div>
<h2 id="lab-4-image-repository">Lab 4 - Image repository</h2>
<p>During these labs we have been using <a href="https://hub.docker.com/">Docker Hub</a> as image repository, but in this lab, we are going to create our own docker image and we are going to push it to OpenShift image repository.</p>
<p>For this Lab we have two options, we can work with a Java EE application using a WebSphere Liberty runtime or a Javascript application using a Node.js runtime.</p>
<div class="admonition warning">
<p class="admonition-title">Note</p>
<p>The administrator of this Lab system should make sure that the <a href="https://docs.openshift.com/container-platform/4.12/registry/securing-exposing-registry.html">OpenShift image registry has been exposed</a> via a route.</p>
</div>
<h3 id="1-create-an-image-nodejs">1. Create an image (Node.js)</h3>
<p>In this lab we will create an image almost from "scratch", we will reuse a base operating system image and we will configure it, install middleware, and install the application. In the next lab we will reuse an image with everything already installed and we will simply deploy the application.</p>
<p>For this lab we provide three files, <strong>app.js</strong> with the node.js code, <strong>package.json</strong> to automate the build and execution of the node.js with npm and <strong>hellonode.buildfile</strong> to generate the image.</p>
<p>In your environment these files should be present in the folder <strong><em>/home/userX/artifacts/nodejs</em></strong>.</p>
<p>This is the content of the <strong>hellonode.buildfile</strong>:</p>
<div class="no-copy highlight"><pre><span></span><code><span class="k">FROM</span><span class="w"> </span><span class="s">debian:10</span>

<span class="k">RUN</span><span class="w"> </span>apt-get<span class="w"> </span>-y<span class="w"> </span>update<span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span>apt-get<span class="w"> </span>install<span class="w"> </span>-y<span class="w"> </span>curl

<span class="k">RUN</span><span class="w"> </span>curl<span class="w"> </span>-sL<span class="w"> </span>https://deb.nodesource.com/setup_20.x<span class="w"> </span><span class="p">|</span><span class="w"> </span>bash<span class="w"> </span>-

<span class="k">RUN</span><span class="w"> </span>apt-get<span class="w"> </span>install<span class="w"> </span>-y<span class="w"> </span>nodejs

<span class="k">RUN</span><span class="w"> </span>mkdir<span class="w"> </span>/app
<span class="k">ADD</span><span class="w"> </span>./nodejs/app.js<span class="w"> </span>/app
<span class="k">ADD</span><span class="w"> </span>./nodejs/package.json<span class="w"> </span>/app
<span class="k">RUN</span><span class="w"> </span><span class="nb">cd</span><span class="w"> </span>/app<span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span>npm<span class="w"> </span>install

<span class="k">ENV</span><span class="w"> </span>PORT<span class="w"> </span><span class="m">8080</span>
<span class="k">EXPOSE</span><span class="w"> </span><span class="s">8080</span>
<span class="k">WORKDIR</span><span class="w"> </span><span class="s">&quot;/app&quot;</span>
<span class="k">CMD</span><span class="w"> </span><span class="p">[</span><span class="s2">&quot;npm&quot;</span><span class="p">,</span><span class="w"> </span><span class="s2">&quot;start&quot;</span><span class="p">]</span>
</code></pre></div>
<p>With this build file what we do is to create a new image from a debian image in docker hub. This is specified by the line </p>
<div class="no-copy highlight"><pre><span></span><code><span class="k">FROM</span><span class="w"> </span><span class="s">debian:10</span>
</code></pre></div>
<p>Then we install <code>node.js runtime v20</code> and copy our application files inside the image in the right directories. </p>
<p>Finally, we specify in the image that it listens to <code>port 8080</code> and that starts with command <code>/app/npm start</code>.</p>
<p><strong>Create the image</strong></p>
<p>First, we need to know the OpenShift internal image registry URL:</p>
<div class="copy highlight"><span class="filename">Command</span><pre><span></span><code>HOST=$(oc get route default-route -n openshift-image-registry --template=&#39;{{ .spec.host }}&#39;)
echo $HOST
</code></pre></div>
<p>It should be something like </p>
<div class="no-copy highlight"><pre><span></span><code>default-route-openshift-image-registry.apps.XXXXXXX.cloud.techzone.ibm.com
</code></pre></div>
<p>Then, execute from the artifacts folder in your home directory the following command to build the docker image (notice last colon in the build command).</p>
<div class="copy highlight"><span class="filename">Command</span><pre><span></span><code>cd /home/userX/artifacts
podman build -t hellonode:1.0 -f ./nodejs/hellonode.buildfile .
</code></pre></div>
<p>It can take some time, it depends on if the image is on the machine or not (if not, it has to be downloaded).</p>
<div class="no-copy output highlight"><span class="filename">Output</span><pre><span></span><code>STEP 1/12: FROM debian:10
Resolved &quot;debian&quot; as an alias (/etc/containers/registries.conf.d/000-shortnames.conf)
Trying to pull docker.io/library/debian:10...
Getting image source signatures
Copying blob sha256:a9a7bf5145e4a86e137c0a7612407b53eb4b97f73f4d6f15a64c2d52c682b500
Copying config sha256:1048ff9a62c68759bfea343a5e6039b909f35107896d5d2ced130d8353464f04
Writing manifest to image destination
STEP 2/12: RUN apt-get -y update &amp;&amp; apt-get install -y curl
...
...
STEP 11/12: WORKDIR &quot;/app&quot;
--&gt; 021a922758f5
STEP 12/12: CMD [&quot;npm&quot;, &quot;start&quot;]
COMMIT hellonode:1.0
--&gt; 083a7cccf60e
Successfully tagged localhost/hellonode:1.0
083a7cccf60e0bce9cb12e1ff107c5559e7fc2f0999d49a2dbd5659444c6b878
</code></pre></div>
<p>You can check the image created:</p>
<div class="copy highlight"><span class="filename">Command</span><pre><span></span><code>podman images
</code></pre></div>
<div class="no-copy output highlight"><span class="filename">Output</span><pre><span></span><code>REPOSITORY           TAG         IMAGE ID      CREATED        SIZE
localhost/hellonode  1.0         083a7cccf60e  2 minutes ago  404 MB
</code></pre></div>
<div class="admonition info">
<p class="admonition-title">Alternative</p>
<p>We have the image you should build in the previous step in <a href="https://hub.docker.com/r/aranchaocana/hellonode">docker.io</a> in case you can not build it yourself because you are behind a proxy or face any other issues. In this step, you pull the image and tag it, the result is the same as if you had executed the build process.</p>
<div class="copy highlight"><span class="filename">Command</span><pre><span></span><code>podman pull docker.io/aranchaocana/hellonode:2.0
podman tag docker.io/aranchaocana/hellonode:2.0 hellonode:1.0
</code></pre></div>
</div>
<p>Now, once you have created the image locally, you are going to tag it to the final repository. Be careful to replace <strong>nsX</strong> on the next command.</p>
<div class="copy highlight"><span class="filename">Command</span><pre><span></span><code>podman tag hellonode:1.0 $HOST/nsX/hellonode:1.0
</code></pre></div>
<div class="admonition info">
<p class="admonition-title">Info</p>
<p>We will explain later the notation for tagging images as <code>$HOST/nsX/image_name:image_tag</code></p>
</div>
<p>Check your images again:</p>
<div class="copy highlight"><span class="filename">Command</span><pre><span></span><code>podman images
</code></pre></div>
<div class="no-copy output highlight"><span class="filename">Output</span><pre><span></span><code>REPOSITORY                                                                                                  TAG         IMAGE ID      CREATED        SIZE
localhost/hellonode                                                                                         1.0         083a7cccf60e  4 minutes ago  404 MB
default-route-openshift-image-registry.apps.65f05030777e25001ee8fdfa.cloud.techzone.ibm.com/ns30/hellonode  1.0         083a7cccf60e  4 minutes ago  404 MB
</code></pre></div>
<h3 id="2-create-an-image-java-ee">2. Create an image (Java EE)</h3>
<p>For this lab we provide two files, <strong>HelloK8s.war</strong> with the Java EE web application and <strong>hellok8s.buildfile</strong> to generate the image.</p>
<p>In your environment these files should be present in the folder <strong><em>/home/userX/artifacts/javaee</em></strong>.</p>
<p>This is the content of the <strong>hellok8s.buildfile</strong>:</p>
<div class="no-copy highlight"><pre><span></span><code><span class="k">FROM</span><span class="w"> </span><span class="s">websphere-liberty:19.0.0.9-webProfile8-java11</span>

<span class="k">COPY</span><span class="w"> </span>--chown<span class="o">=</span><span class="m">1001</span>:0<span class="w"> </span>./javaee/HelloK8s.war<span class="w"> </span>/config/dropins/
</code></pre></div>
<p>With this file what we do is to create a new image from an existing WebSphere Liberty installation with only the modules needed by the Web Profile specifications and using Java SE 11. This is specified by the line</p>
<div class="no-copy highlight"><pre><span></span><code><span class="k">FROM</span><span class="w"> </span><span class="s">websphere-liberty:19.0.0.9-webProfile8-java11</span>
</code></pre></div>
<p>Then we copy our application file inside the image in the right directory.</p>
<p><strong>Create the image</strong></p>
<p>Execute from the artifacts folder in your home directory the following command to build the docker image (notice last colon in the build command).</p>
<div class="copy highlight"><span class="filename">Command</span><pre><span></span><code>cd /home/userX/artifacts/
podman build -t hellok8s:1.0 -f ./javaee/hellok8s.buildfile .
</code></pre></div>
<div class="no-copy output highlight"><span class="filename">Output</span><pre><span></span><code>STEP 1/2: FROM websphere-liberty:19.0.0.9-webProfile8-java11
Resolving &quot;websphere-liberty&quot; using unqualified-search registries (/etc/containers/registries.conf.d/999-podman-machine.conf)
Trying to pull docker.io/library/websphere-liberty:19.0.0.9-webProfile8-java11...
Getting image source signatures
Copying blob sha256:849d56b8f8ce5eac04ac1618c9e81a0f4bf30d7b1a1159fd0641112eb77d6554
Copying blob sha256:de83a2304fa1f7c4a13708a0d15b9704f5945c2be5cbb2b3ed9b2ccb718d0b3d
Copying blob sha256:423ae2b273f4c17ceee9e8482fa8d071d90c7d052ae208e1fe4963fceb3d6954
Copying blob sha256:b6b53be908de2c0c78070fff0a9f04835211b3156c4e73785747af365e71a0d7
Copying blob sha256:f9a83bce3af0648efaa60b9bb28225b09136d2d35d0bed25ac764297076dec1b
Copying blob sha256:d6eb45629ecf6b86562f04def09260db9e15101e2a6974e746e19e5077c1f605
Copying blob sha256:d22178cfc3346cf1fa48ef7990423b33344cceec8ba9d1b581238b29e1eb50e7
Copying blob sha256:dc72c60911b444e1b85040e3038b67ee9b8deddc1e6791aafcca484128a71b92
Copying blob sha256:783927643a0ed0ad0ce818a6f126d12ff06e88a97e2dd3a0a5d10e7094b721e6
Copying blob sha256:0f7666177f3b860ba093cec48942b4fa93a9b7450bd31e5b7770dcbe2848e67a
Copying blob sha256:975c2ce1319ffc00b1d345d142db0589f04d3dfed194d60af5e53bc44ce94d2b
Copying blob sha256:bd6a269b8c04c40e3222d332a4a7a84bfa3faab833255012d6fca8041f122928
Copying blob sha256:ca09d98aadadafb33f488cad3c3d7bdc8c5f1ca4038c406d8d6dcf0ce492124d
Copying blob sha256:f7867820d612db1130128c6af14ae7f1c1215565ccacba671f095c0f7aa39f7a
Copying blob sha256:cb6a1680efbbd0058049151ac11c3ae131cfe8665f024cde14efbe10692e0a23
Copying config sha256:40271528d14a21bb12e44105be8cf7bf7bff50f4a44dbc6836dd7252efcb0d38
Writing manifest to image destination
STEP 2/2: COPY --chown=1001:0 ./javaee/HelloK8s.war /config/dropins/
COMMIT hellok8s:1.0
--&gt; f6e9efdcd944
Successfully tagged localhost/hellok8s:1.0
f6e9efdcd944783ed1f5bd5272a8c71058101b9e2347e5bce5792d5f15300c32
</code></pre></div>
<p>You can check the image created:</p>
<div class="copy highlight"><span class="filename">Command</span><pre><span></span><code>podman images
</code></pre></div>
<div class="no-copy output highlight"><span class="filename">Output</span><pre><span></span><code>REPOSITORY                                                                                                  TAG                          IMAGE ID      CREATED         SIZE
localhost/hellok8s                                                                                          1.0                          f6e9efdcd944  33 seconds ago  396 MB
localhost/hellonode                                                                                         1.0                          083a7cccf60e  6 minutes ago   404 MB
default-route-openshift-image-registry.apps.65f05030777e25001ee8fdfa.cloud.techzone.ibm.com/ns30/hellonode  1.0                          083a7cccf60e  6 minutes ago   404 MB
docker.io/library/websphere-liberty                                                                         19.0.0.9-webProfile8-java11  40271528d14a  4 years ago     396 MB
</code></pre></div>
<div class="admonition info">
<p class="admonition-title">Alternative</p>
<p>We have the image you should build in the previous step in <a href="https://hub.docker.com/r/aranchaocana/hellok8s-java">docker.io</a> in case you can not build it yourself because you are behind a proxy or face any other issues. In this step, you pull the image and tag it, the result is the same as if you had executed the build process.</p>
<div class="copy highlight"><span class="filename">Command</span><pre><span></span><code>podman pull docker.io/aranchaocana/hellok8s-java:1.0
podman tag docker.io/aranchaocana/hellok8s-java:1.0 hellok8s:1.0
</code></pre></div>
</div>
<p>Now, once you have created the image locally, you are going to tag it to the final repository. Be careful to replace <strong>nsX</strong> on the next command.</p>
<div class="copy highlight"><span class="filename">Command</span><pre><span></span><code>podman tag hellok8s:1.0 $HOST/nsX/hellok8s:1.0
</code></pre></div>
<p>Check your images again:</p>
<div class="copy highlight"><span class="filename">Command</span><pre><span></span><code>podman images
</code></pre></div>
<div class="no-copy output highlight"><span class="filename">Output</span><pre><span></span><code>REPOSITORY                                                                                                  TAG                          IMAGE ID      CREATED        SIZE
localhost/hellok8s                                                                                          1.0                          f6e9efdcd944  2 minutes ago  396 MB
default-route-openshift-image-registry.apps.65f05030777e25001ee8fdfa.cloud.techzone.ibm.com/ns30/hellok8s   1.0                          f6e9efdcd944  2 minutes ago  396 MB
localhost/hellonode                                                                                         1.0                          083a7cccf60e  8 minutes ago  404 MB
default-route-openshift-image-registry.apps.65f05030777e25001ee8fdfa.cloud.techzone.ibm.com/ns30/hellonode  1.0                          083a7cccf60e  8 minutes ago  404 MB
docker.io/library/websphere-liberty                                                                         19.0.0.9-webProfile8-java11  40271528d14a  4 years ago    396 MB
</code></pre></div>
<h3 id="3-publish-the-image-to-openshift">3. Publish the image to OpenShift</h3>
<p>We will publish the <strong>hellonode</strong> and <strong>hellok8s</strong> images to the OpenShift internal image registry using the <strong>podman push</strong> command. But before that, we should make sure the image tagging is correct.</p>
<p><strong>Tag the image:</strong></p>
<p>Podman will know where to push the image based on the image tag. Remember our tag is:</p>
<div class="no-copy highlight"><pre><span></span><code>default-route-openshift-image-registry.apps.XXXXXXX.cloud.techzone.ibm.com/nsX/hellok8s:1.0
</code></pre></div>
<p>Where:</p>
<ul>
<li><code>default-route-openshift-image-registry.apps.XXXXXXX.cloud.techzone.ibm.com</code> is the docker image registry.</li>
<li><code>nsX</code> is the namespace.</li>
<li><code>hellok8s</code> is the image name.</li>
<li><code>1.0</code> is the version of the image.</li>
</ul>
<p><strong>Login to the repository and push the image:</strong></p>
<p>Execute these podman commands:</p>
<div class="copy highlight"><span class="filename">Command</span><pre><span></span><code>podman login $HOST -u $(oc whoami) -p $(oc whoami -t)
podman push $HOST/nsX/hellonode:1.0 $HOST/nsX/hellonode:1.0
podman push $HOST/nsX/hellok8s:1.0 $HOST/nsX/hellok8s:1.0
</code></pre></div>
<p>Now in the OpenShift console you can see these images. Go to left menu panel and open the <strong>Builds</strong> section, then select <strong>Image Streams</strong>. Within each Image Stream, you will see all the tags (or versions) existing in the OpenShift internal image registry:</p>
<p><img alt="4.3.01" src="images/app-mod-ocp/4.3.01.png" style="max-height:600px" /></p>
<p><img alt="4.3.02" src="images/app-mod-ocp/4.3.02.png" style="max-height:700px" /></p>
<h3 id="4-create-a-deployment">4. Create a Deployment</h3>
<p>As we did in previous Labs in this document, we can create a deployment using this pushed image. </p>
<p>At this point we have two options, use the OpenShift internal image registry internal URL (<strong>image-registry.openshift-image-registry.svc:5000</strong>) or the external one we used before to push the images. The external route, however, would require a pullsecret with the credentials to authenticate against it as we did with podman login. Thus, Let's use the internal URL that is easier and faster.</p>
<p>This is the yaml file for the deployment <strong>was-deployment-registry1.yaml</strong>:</p>
<div class="copy highlight"><pre><span></span><code><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">apps/v1</span>
<span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Deployment</span>
<span class="nt">metadata</span><span class="p">:</span>
<span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">websphere-liberty-app</span>
<span class="hll"><span class="w">  </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">nsX</span>
</span><span class="nt">spec</span><span class="p">:</span>
<span class="w">  </span><span class="nt">selector</span><span class="p">:</span>
<span class="w">    </span><span class="nt">matchLabels</span><span class="p">:</span>
<span class="w">      </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">websphere-liberty-app</span>
<span class="w">  </span><span class="nt">replicas</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span>
<span class="w">  </span><span class="nt">template</span><span class="p">:</span>
<span class="w">    </span><span class="nt">metadata</span><span class="p">:</span>
<span class="w">      </span><span class="nt">labels</span><span class="p">:</span>
<span class="w">        </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">websphere-liberty-app</span>
<span class="w">    </span><span class="nt">spec</span><span class="p">:</span>
<span class="w">      </span><span class="nt">containers</span><span class="p">:</span>
<span class="w">        </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">websphere-liberty-app</span>
<span class="hll"><span class="w">          </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">image-registry.openshift-image-registry.svc:5000/nsX/hellok8s:1.0</span>
</span><span class="w">          </span><span class="nt">ports</span><span class="p">:</span>
<span class="w">            </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">containerPort</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">9080</span>
</code></pre></div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>The <strong>indentation</strong> is very important. Pay attention when executing the copy and paste. If you have problems with the copy and paste, <strong>all the files used in the labs are in the artifact folder under the user home folder</strong>.</p>
</div>
<p>To create the deployment, execute the following command:</p>
<div class="copy highlight"><span class="filename">Command</span><pre><span></span><code>oc create -f was-deployment-registry1.yaml
</code></pre></div>
<p>Also, create the service and the ingress that is required in order to be able to access the application from outside the OpenShift cluster (since we are using the same application name, the service and ingress definitions from earlier can be reused):</p>
<div class="copy highlight"><span class="filename">Command</span><pre><span></span><code>oc create -f was-service1.yaml
oc create -f was-ingress.yaml
</code></pre></div>
<p>As we have created an application on Websphere Liberty, we have to use the context root for the application (HelloK8s) to be able to access it. Point your browser to:</p>
<p><a href="http://websphere-liberty-app-nsX.apps.XXXXXXX.cloud.techzone.ibm.com/HelloK8s">http://websphere-liberty-app-nsX.apps.XXXXXXX.cloud.techzone.ibm.com/HelloK8s</a></p>
<p><img alt="4.4.01" src="images/app-mod-ocp/4.4.01.png" style="max-height:100px" /></p>
<p>You can also use <strong>curl</strong> to test your application:</p>
<div class="copy highlight"><span class="filename">Command</span><pre><span></span><code>curl<span class="w"> </span>http://websphere-liberty-app-nsX.apps.XXXXXXX.cloud.techzone.ibm.com/HelloK8s
</code></pre></div>
<div class="no-copy output highlight"><span class="filename">Output</span><pre><span></span><code>I&#39;m alive!!!
</code></pre></div>
<p>The other option is using the OpenShift internal image registry external URL. As already mentioned, we would first need to create a secret with the appropriate authentication credentials. </p>
<p>Let's also explore this case as it is very useful to understand how to use an external registry.</p>
<p>Let's create a secret executing this command:</p>
<div class="copy highlight"><span class="filename">Command</span><pre><span></span><code>oc create secret docker-registry exposed-openshift-registry \
  -n nsX \
  --docker-server=$HOST \
  --docker-username=$(oc whoami) \
  --docker-password=$(oc whoami -t) \
  --docker-email=userX@openshift.pot
</code></pre></div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Remember that the user and the password to authenticate with the OpenShift internal image registry can be retrieved respectively with</p>
<ul>
<li><code>oc whoami</code></li>
<li><code>oc whoami -t</code></li>
</ul>
</div>
<p>In the deployment, we use as the image registry the exposed host of the OpenShift internal image registry, and we configure the property <strong>imagePullSecrets</strong> with the secret we have just created to authenticate against the repository.</p>
<p>Create file <strong>was-deployment-registry2.yaml</strong>:</p>
<div class="copy highlight"><pre><span></span><code><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">apps/v1</span>
<span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Deployment</span>
<span class="nt">metadata</span><span class="p">:</span>
<span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">websphere-liberty-app</span>
<span class="hll"><span class="w">  </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">nsX</span>
</span><span class="nt">spec</span><span class="p">:</span>
<span class="w">  </span><span class="nt">selector</span><span class="p">:</span>
<span class="w">    </span><span class="nt">matchLabels</span><span class="p">:</span>
<span class="w">      </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">websphere-liberty-app</span>
<span class="w">  </span><span class="nt">replicas</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span>
<span class="w">  </span><span class="nt">template</span><span class="p">:</span>
<span class="w">    </span><span class="nt">metadata</span><span class="p">:</span>
<span class="w">      </span><span class="nt">labels</span><span class="p">:</span>
<span class="w">        </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">websphere-liberty-app</span>
<span class="w">    </span><span class="nt">spec</span><span class="p">:</span>
<span class="w">      </span><span class="nt">containers</span><span class="p">:</span>
<span class="w">        </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">websphere-liberty-app</span>
<span class="hll"><span class="w">          </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">default-route-openshift-image-registry.apps.XXXXXXX.cloud.techzone.ibm.com/nsX/hellok8s:1.0</span>
</span><span class="w">          </span><span class="nt">ports</span><span class="p">:</span>
<span class="w">            </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">containerPort</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">9080</span>
<span class="w">      </span><span class="nt">imagePullSecrets</span><span class="p">:</span>
<span class="w">        </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">exposed-openshift-registry</span>
</code></pre></div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>The <strong>indentation</strong> is very important. Pay attention when executing the copy and paste. If you have problems with the copy and paste, <strong>all the files used in the labs are in the artifact folder under the user home folder</strong>.</p>
</div>
<p>Once you have the file updated, you can update the application via the apply command:</p>
<div class="copy highlight"><span class="filename">Command</span><pre><span></span><code>oc apply -f was-deployment-registry2.yaml
</code></pre></div>
<div class="no-copy output highlight"><span class="filename">Output</span><pre><span></span><code>deployment.apps/websphere-liberty-app configured
</code></pre></div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Disregard any warning yield as a result of using <code>oc apply -f</code></p>
</div>
<p>Check the status of the pod:</p>
<div class="copy highlight"><span class="filename">Command</span><pre><span></span><code>oc get pods -n nsX
</code></pre></div>
<div class="no-copy output highlight"><span class="filename">Output</span><pre><span></span><code>NAME                                     READY   STATUS    RESTARTS   AGE
websphere-liberty-app-848fc68d97-2q7w5   1/1     Running   0          53s
</code></pre></div>
<p>Now, point your browser to:</p>
<p><a href="http://websphere-liberty-app-nsX.apps.XXXXXXX.cloud.techzone.ibm.com/HelloK8s">http://websphere-liberty-app-nsX.apps.XXXXXXX.cloud.techzone.ibm.com/HelloK8s</a></p>
<p><img alt="4.4.01" src="images/app-mod-ocp/4.4.01.png" style="max-height:100px" /></p>
<p>the output is the same as before, but this time we are using an image from an <strong>"external"</strong> docker image registry.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>We have simulated the behavior as if it was a real external docker image registry.</p>
</div>
<p>You can also execute the following curl command:</p>
<div class="copy highlight"><span class="filename">Command</span><pre><span></span><code>curl<span class="w"> </span>http://websphere-liberty-app-nsX.apps.XXXXXXX.cloud.techzone.ibm.com/HelloK8s
</code></pre></div>
<div class="no-copy output highlight"><span class="filename">Output</span><pre><span></span><code>I&#39;m alive!!!
</code></pre></div>
<h3 id="5-clean-all-the-artifacts_1">5. Clean all the artifacts</h3>
<div class="copy highlight"><span class="filename">Command</span><pre><span></span><code>oc delete -f was-deployment-registry2.yaml
oc delete -f was-service1.yaml
oc delete -f was-ingress.yaml
oc delete secret exposed-openshift-registry
</code></pre></div>
<h2 id="lab-5-application-management">Lab 5 - Application Management</h2>
<p>Let's start deploying the Java EE or the Node.js application with its services to be accessible.</p>
<div class="admonition info">
<p class="admonition-title">Info</p>
<p>All the commands and screenshots will be using Java EE sample, but it works the same with Node.js</p>
</div>
<div class="copy highlight"><span class="filename">Command</span><pre><span></span><code>oc create -f node-deployment-registry1.yaml
oc create -f node-service1.yaml
oc create -f node-ingress.yaml
</code></pre></div>
<p>Then, list all pods executing:</p>
<div class="copy highlight"><span class="filename">Command</span><pre><span></span><code>oc get pods -n nsx
</code></pre></div>
<p>Verify that <strong>node-app</strong> pod has not been restarted anytime. <strong>RESTARTS</strong> value should be <strong>0</strong>.</p>
<div class="no-copy output highlight"><span class="filename">Output</span><pre><span></span><code>NAME                        READY   STATUS    RESTARTS   AGE
node-app-59fdfd5d69-vqphh   1/1     Running   0          14s
</code></pre></div>
<h3 id="1-automatic-restarts">1. Automatic restarts</h3>
<p>Let's invoke <strong>/kill</strong> endpoint in the sample application. The <strong>/kill</strong> method executes <code>System.exit(0);</code> in Java or <code>process.exit();</code> in Node.js, which makes both runtimes to quit and finish the container.</p>
<p>First, load the application in the browser via this URL: </p>
<p><a href="http://node-app-nsX.apps.XXXXXXX.cloud.techzone.ibm.com/">http://node-app-nsX.apps.XXXXXXX.cloud.techzone.ibm.com/</a></p>
<p><img alt="5.1.01" src="images/app-mod-ocp/5.1.01.png" style="max-height:120px" /></p>
<p>Or from the command line:</p>
<div class="copy highlight"><span class="filename">Command</span><pre><span></span><code>curl http://node-app-nsX.apps.XXXXXXX.cloud.techzone.ibm.com/
</code></pre></div>
<div class="no-copy output highlight"><span class="filename">Output</span><pre><span></span><code>I&#39;m alive 8!!!
</code></pre></div>
<p>Once the app is running, call the <strong>/kill</strong> endpoint:</p>
<p><a href="http://node-app-nsX.apps.XXXXXXX.cloud.techzone.ibm.com/kill">http://node-app-nsX.apps.XXXXXXX.cloud.techzone.ibm.com/kill</a></p>
<p><img alt="5.1.02" src="images/app-mod-ocp/5.1.02.png" style="max-height:120px" /></p>
<p>Or executing the respective curl command: </p>
<div class="copy highlight"><span class="filename">Command</span><pre><span></span><code>curl http://node-app-nsX.apps.XXXXXXX.cloud.techzone.ibm.com/kill
</code></pre></div>
<p>After invoking <strong>/kill</strong>, if you check the running pods in our namespace, you will see that <strong>node-app</strong> pod is still running but now RESTARTS field has value <strong>1</strong>. If you execute the commands very fast, you will be able to see how the old pod is completed and a new one is started.</p>
<p>This is happening because in the deployment specification of the application we set "replicas" to <strong>1</strong>. This means, we want always one instance running.</p>
<div class="copy highlight"><span class="filename">Command</span><pre><span></span><code>oc get pods -n nsx
</code></pre></div>
<div class="no-copy output highlight"><span class="filename">Output</span><pre><span></span><code>NAME                        READY   STATUS    RESTARTS      AGE
node-app-59fdfd5d69-vqphh   1/1     Running   1 (39s ago)   2m49s
</code></pre></div>
<p>We can check again this behavior but deleting the pod manually from the command line, instead of killing the container via the sample application logic:</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Make sure to change the pod name <code>node-app-59fdfd5d69-vqphh</code> with the name of your pod from the previous steps.</p>
</div>
<div class="copy highlight"><span class="filename">Command</span><pre><span></span><code>oc delete pod node-app-59fdfd5d69-vqphh -n nsX
</code></pre></div>
<p>Then, check the restarts of the pod</p>
<div class="copy highlight"><span class="filename">Command</span><pre><span></span><code>oc get pods -n nsx
</code></pre></div>
<div class="no-copy output highlight"><span class="filename">Output</span><pre><span></span><code>NAME                        READY   STATUS    RESTARTS   AGE
node-app-59fdfd5d69-fjzmc   1/1     Running   0          4s
</code></pre></div>
<p>You can see that a new pod (the name is different) has been created instead of restarting the old one, so <strong>RESTARTS</strong> field has value <strong>0</strong> again. This is because we have deleted the pod. In previous sample, the same pod was restarted when its container was killed (not deleted). That is the difference, but the final behavior from end user perspective is the same, OpenShift has maintained automatically our application running so we get resiliency out of the box.</p>
<h3 id="2-query-application-health">2. Query application health</h3>
<p>There are sometimes where the container is running but the application is in trouble. We want OpenShift to ask the application its status, and if it doesn't return a <strong>HTTP 200 OK</strong> code, OpenShift will restart the application.</p>
<p>In our sample application we have two other endpoints:</p>
<ul>
<li><code>/health</code>: will check an instance variable and return <code>HTTP 200</code> or <code>HTTP 500</code> based on its value.</li>
<li><code>/infect</code>: will change the instance variable value to force <code>HTTP 500</code> in <code>/health</code> endpoint.</li>
</ul>
<p>So, after invoking the <code>/infect</code> endpoint, the application will start returning <code>HTTP 500</code> code on its <code>/health</code> endpoint.</p>
<p>Let's add that health check to our deployment descriptor. We can edit the YAML file and apply the change or edit it from the OpenShift console. Let's do this second option this time to learn a new way of modifying deployed elements.</p>
<div class="admonition info">
<p class="admonition-title">Info</p>
<p>Of course, all the changes applied through the OpenShift console will be lost if we start from scratch with the original deployment YAML files.</p>
</div>
<p>Go to the web console and select from the left menu <strong>Workloads</strong> and <strong>Deployments</strong>. You should see our deployed application.</p>
<p><img alt="5.2.01" src="images/app-mod-ocp/5.2.01.png" style="max-height:500px" /></p>
<p>Click on it and select the YAML tab.</p>
<p><img alt="5.2.02" src="images/app-mod-ocp/5.2.02.png" style="max-height:500px" /></p>
<p>We will see a lot of new fields that we didn't have in our original YAML file. All those fields are added automatically by OpenShift. But our original fields are all present.</p>
<p>Modify the deployment descriptor to add the field <strong>livenessProbe</strong>. This element is used by OpenShift to poll the application asking for its status, it can be an HTTP request, TCP invocation or an shell command. We are going to use an HTTP check: if the response has a status code less than <code>200</code> or greater than <code>400</code>, the pod will be restarted automatically.</p>
<p>However, for OpenShift check the <strong>livenessProbe</strong>, the <strong>readinessProbe</strong> should be checked before and return <strong>ready</strong> status.</p>
<p>These are the specs for livenessProbe and readinessProbe we need to place properly in our deployment definition:</p>
<div class="copy highlight"><pre><span></span><code><span class="w">          </span><span class="nt">readinessProbe</span><span class="p">:</span>
<span class="w">            </span><span class="nt">httpGet</span><span class="p">:</span>
<span class="w">              </span><span class="nt">path</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">/health</span>
<span class="w">              </span><span class="nt">port</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">8080</span>
<span class="w">              </span><span class="nt">scheme</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">HTTP</span>
<span class="w">            </span><span class="nt">initialDelaySeconds</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">5</span>
<span class="w">            </span><span class="nt">timeoutSeconds</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">30</span>
<span class="w">            </span><span class="nt">periodSeconds</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">10</span>
<span class="w">            </span><span class="nt">successThreshold</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span>
<span class="w">            </span><span class="nt">failureThreshold</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">3</span><span class="w">            </span>
<span class="w">          </span><span class="nt">livenessProbe</span><span class="p">:</span>
<span class="w">            </span><span class="nt">httpGet</span><span class="p">:</span>
<span class="w">              </span><span class="nt">path</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">/health</span>
<span class="w">              </span><span class="nt">port</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">8080</span>
<span class="w">              </span><span class="nt">scheme</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">HTTP</span>
<span class="w">            </span><span class="nt">initialDelaySeconds</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">5</span>
<span class="w">            </span><span class="nt">timeoutSeconds</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">30</span>
<span class="w">            </span><span class="nt">periodSeconds</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">10</span>
<span class="w">            </span><span class="nt">successThreshold</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span>
<span class="w">            </span><span class="nt">failureThreshold</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">3</span>
</code></pre></div>
<div class="admonition warning">
<p class="admonition-title">Important</p>
<p>Make sure the indentation is set properly. Otherwise, OpenShift will yield errors.</p>
</div>
<p>Your deployment should look like this (disregard properties order, check the readiness and liveness probes are well indented in relation to the other properties):</p>
<p><img alt="5.2.03" src="images/app-mod-ocp/5.2.03.png" style="max-height:800px" /></p>
<p>Then press <strong>Save</strong> button and <strong>Reload</strong>. </p>
<p>If you prefer to edit it using vi, you can execute this command, but be careful with the indentation that might be different:</p>
<div class="copy highlight"><span class="filename">Command</span><pre><span></span><code>oc edit deployment node-app -n nsX
</code></pre></div>
<p>With this configuration, OpenShift will invoke the endpoint <strong>/heath</strong> in intervals of 10 seconds. If the application does not return HTTP 200 OK for three consecutive times, OpenShift will restart the application.</p>
<p>Load again the application in the browser:</p>
<p><a href="http://node-app-nsX.apps.XXXXXXX.cloud.techzone.ibm.com/">http://node-app-nsX.apps.XXXXXXX.cloud.techzone.ibm.com/</a></p>
<p>or execute</p>
<div class="copy highlight"><span class="filename">Command</span><pre><span></span><code>curl http://node-app-nsX.apps.XXXXXXX.cloud.techzone.ibm.com/
</code></pre></div>
<p>List existing pods and take note of the number of restarts:</p>
<div class="copy highlight"><span class="filename">Command</span><pre><span></span><code>oc get pods -n nsX
</code></pre></div>
<p>Check the logs of the application:</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Make sure to change the pod name <code>node-app-58f9494cdf-kf7k7</code> with the name of your pod from the previous step.</p>
</div>
<div class="copy highlight"><span class="filename">Command</span><pre><span></span><code>oc logs -f node-app-58f9494cdf-kf7k7 -n nsX
</code></pre></div>
<div class="no-copy output highlight"><span class="filename">Output</span><pre><span></span><code>&gt; HelloKube@1.0.0 start
&gt; node app.js

[2024-03-27 16:12:37.840] [INFO] HelloKube - Starting...
[2024-03-27 16:12:37.847] [INFO] HelloKube - HelloKube listening on port 8080
[2024-03-27 16:12:40.847] [INFO] HelloKube - lalala....
[2024-03-27 16:12:43.850] [INFO] HelloKube - lalala....
[2024-03-27 16:12:46.851] [INFO] HelloKube - lalala....
[2024-03-27 16:12:47.090] [INFO] HelloKube - operation /health invoked... returning GREEN
(node:18) [DEP0066] DeprecationWarning: OutgoingMessage.prototype._headers is deprecated
(Use `node --trace-deprecation ...` to show where the warning was created)
[2024-03-27 16:12:47.097] [INFO] HelloKube - operation /health invoked... returning GREEN
[2024-03-27 16:12:49.853] [INFO] HelloKube - lalala....
[2024-03-27 16:12:52.856] [INFO] HelloKube - lalala....
[2024-03-27 16:12:55.860] [INFO] HelloKube - lalala....
[2024-03-27 16:12:57.085] [INFO] HelloKube - operation /health invoked... returning GREEN
[2024-03-27 16:12:57.086] [INFO] HelloKube - operation /health invoked... returning GREEN
[2024-03-27 16:12:58.861] [INFO] HelloKube - lalala....
[2024-03-27 16:13:01.865] [INFO] HelloKube - lalala....
</code></pre></div>
<p>You can see how the <strong>/health</strong> endpoint is invoked periodically.</p>
<p>Now if we invoke the <strong>/infect</strong> endpoint from the browser:</p>
<p><a href="http://node-app-nsX.apps.XXXXXXX.cloud.techzone.ibm.com/infect">http://node-app-nsX.apps.XXXXXXX.cloud.techzone.ibm.com/infect</a></p>
<p><img alt="5.2.04" src="images/app-mod-ocp/5.2.04.png" style="max-height:120px" /></p>
<p>or executing:</p>
<div class="copy highlight"><span class="filename">Command</span><pre><span></span><code>curl http://node-app-nsX.apps.XXXXXXX.cloud.techzone.ibm.com/infect
</code></pre></div>
<p>We will see how the logs will change. The endpoint <strong>/health</strong> is now returning <strong>HTTP 500</strong> and after three consecutive invocations with this return code the pod will be restarted.</p>
<div class="no-copy output highlight"><span class="filename">Output</span><pre><span></span><code>...
[2024-03-27 16:15:47.084] [INFO] HelloKube - operation /health invoked... returning GREEN
[2024-03-27 16:15:47.085] [INFO] HelloKube - operation /health invoked... returning GREEN
[2024-03-27 16:15:49.991] [INFO] HelloKube - lalala....
[2024-03-27 16:15:52.995] [INFO] HelloKube - lalala....
<span class="hll">[2024-03-27 16:15:55.396] [INFO] HelloKube - operation /infect invoked...
</span>[2024-03-27 16:15:55.995] [INFO] HelloKube - cough...
[2024-03-27 16:15:57.084] [INFO] HelloKube - operation /health invoked... returning RED
[2024-03-27 16:15:57.086] [INFO] HelloKube - operation /health invoked... returning RED
[2024-03-27 16:15:58.996] [INFO] HelloKube - cough...
[2024-03-27 16:16:01.998] [INFO] HelloKube - cough...
[2024-03-27 16:16:05.000] [INFO] HelloKube - cough...
[2024-03-27 16:16:07.085] [INFO] HelloKube - operation /health invoked... returning RED
[2024-03-27 16:16:07.086] [INFO] HelloKube - operation /health invoked... returning RED
[2024-03-27 16:16:08.001] [INFO] HelloKube - cough...
[2024-03-27 16:16:11.005] [INFO] HelloKube - cough...
[2024-03-27 16:16:14.009] [INFO] HelloKube - cough...
[2024-03-27 16:16:17.012] [INFO] HelloKube - cough...
[2024-03-27 16:16:17.085] [INFO] HelloKube - operation /health invoked... returning RED
[2024-03-27 16:16:17.086] [INFO] HelloKube - operation /health invoked... returning RED
[2024-03-27 16:16:17.086] [INFO] HelloKube - operation /health invoked... returning RED
npm ERR! path /app
npm ERR! command failed
npm ERR! signal SIGTERM
npm ERR! command sh -c node app.js

npm ERR! A complete log of this run can be found in: /root/.npm/_logs/2024-03-27T16_12_37_692Z-debug-0.log
</code></pre></div>
<p>Notice also that the communications (hostnames, ports, etc…) don't change after restarting the pods. That was the objective of the OpenShift service objects we created in previous labs, they automatically reflect and balance the traffic to the application configured in the service selector wherever it is listening now.</p>
<h3 id="3-manual-scaling">3. Manual scaling</h3>
<p>We can modify the number of replicas specified in the deployment any time via web console or command line. For example, open the web console and select under <strong>Workloads</strong> section, <strong>Deployments</strong> option.</p>
<p>Click on our node-app deployment and press the up arrow twice:</p>
<p><img alt="5.3.01" src="images/app-mod-ocp/5.3.01.png" style="max-height:600px" /></p>
<p>You can also edit the deployment yaml with the UI or the CLI and change the <strong>replicas</strong> parameter or there is a specific command to scale:</p>
<div class="copy highlight"><span class="filename">Command</span><pre><span></span><code>oc scale deployment/node-app --replicas=3 -n nsX
</code></pre></div>
<p>Now check available pods, you should see three:</p>
<div class="copy highlight"><span class="filename">Command</span><pre><span></span><code>oc get pods -n nsX | grep node
</code></pre></div>
<div class="no-copy output highlight"><span class="filename">Output</span><pre><span></span><code>NAME                        READY   STATUS    RESTARTS        AGE
node-app-58f9494cdf-kf7k7   1/1     Running   1 (2m23s ago)   6m4s
node-app-58f9494cdf-t4xsw   1/1     Running   0               18s
node-app-58f9494cdf-tk2nb   1/1     Running   0               17s
</code></pre></div>
<p>Downscale again to <strong>1</strong> before continuing.</p>
<h3 id="4-automatic-scaling">4. Automatic scaling</h3>
<p>OpenShift supports the creation of auto-scaling policies based on CPU or RAM consumption and since last versions also based on custom metrics.</p>
<p>Before creating a new policy based on CPU consumption for example, we need to set some resource parameters at deployment level to be able to react based on consumptions.</p>
<p>By default, this parameter is set, in the yaml you will see: </p>
<p><img alt="5.4.01" src="images/app-mod-ocp/5.4.01.png" style="max-height:250px" /></p>
<div class="admonition info">
<p class="admonition-title">Info</p>
<p>Your spec section could look different as the order of properties might differ from deployment to deployment. Focus on what is inside the <strong>resources</strong> section.</p>
</div>
<p>Replace it with the following yaml to request specific CPU and RAM resources for the application. Pay attention to the indentation:</p>
<div class="copy highlight"><pre><span></span><code><span class="w">          </span><span class="nt">resources</span><span class="p">:</span>
<span class="w">            </span><span class="nt">requests</span><span class="p">:</span>
<span class="w">              </span><span class="nt">cpu</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">50m</span>
<span class="w">              </span><span class="nt">memory</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">100Mi</span>
<span class="w">            </span><span class="nt">limits</span><span class="p">:</span>
<span class="w">              </span><span class="nt">cpu</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">300m</span>
<span class="w">              </span><span class="nt">memory</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">300Mi</span>
</code></pre></div>
<p><img alt="5.4.02" src="images/app-mod-ocp/5.4.02.png" style="max-height:1000px" /></p>
<p><strong>Save</strong> and <strong>Reload</strong> in order to see the latest release. </p>
<p>If you prefer to edit it using vi, you can execute this command, but be careful with the indentation that might be different:</p>
<div class="copy highlight"><span class="filename">Command</span><pre><span></span><code>oc edit deployment node-app -n nsX
</code></pre></div>
<p>Double check a new pod has been created. It means values has been applied.</p>
<p>You can see on events tab, deployment-controller do some actions.</p>
<p>Now, let's create <strong>node-hpa.yaml</strong> file with this content:</p>
<div class="copy highlight"><pre><span></span><code><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">autoscaling/v2</span>
<span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">HorizontalPodAutoscaler</span>
<span class="nt">metadata</span><span class="p">:</span>
<span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">node-hpa</span>
<span class="hll"><span class="w">  </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">nsX</span>
</span><span class="nt">spec</span><span class="p">:</span>
<span class="w">  </span><span class="nt">scaleTargetRef</span><span class="p">:</span>
<span class="w">    </span><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">apps/v1</span>
<span class="w">    </span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Deployment</span>
<span class="w">    </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">node-app</span>
<span class="w">  </span><span class="nt">minReplicas</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span>
<span class="w">  </span><span class="nt">maxReplicas</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">3</span>
<span class="w">  </span><span class="nt">metrics</span><span class="p">:</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Resource</span>
<span class="w">      </span><span class="nt">resource</span><span class="p">:</span>
<span class="w">        </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">cpu</span>
<span class="w">        </span><span class="nt">target</span><span class="p">:</span>
<span class="w">          </span><span class="nt">averageUtilization</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">40</span>
<span class="w">          </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Utilization</span>
</code></pre></div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>The <strong>indentation</strong> is very important. Pay attention when executing the copy and paste. If you have problems with the copy and paste, <strong>all the files used in the labs are in the artifact folder under the user home folder</strong>.</p>
</div>
<p>Once you have the file updated, execute:</p>
<div class="copy highlight"><span class="filename">Command</span><pre><span></span><code>oc create -f node-hpa.yaml
</code></pre></div>
<p>You can also do it via Web browser, <code>Workload -&gt; Horizontal Pod AutoScalers -&gt; Create Horizontal Pod Autoscaler</code> (and change the generated code with ours)</p>
<p><img alt="5.4.03" src="images/app-mod-ocp/5.4.03.png" style="max-height:700px" /></p>
<p>Get the values of the hpa. It takes time to get the current metrics, but you can see at least the target:</p>
<div class="copy highlight"><span class="filename">Command</span><pre><span></span><code>oc get hpa/node-hpa
</code></pre></div>
<div class="no-copy output highlight"><span class="filename">Output</span><pre><span></span><code>NAME       REFERENCE             TARGETS         MINPODS   MAXPODS   REPLICAS   AGE
node-hpa   Deployment/node-app   &lt;unknown&gt;/40%   1         3         1          79s
</code></pre></div>
<p>Now we are going to query the application in an endless loop to generate enough load and force that the CPU consumed by the application grows beyond the specified 40%. Open a new terminal and execute this shell script. Leave it running:</p>
<div class="copy highlight"><span class="filename">Command</span><pre><span></span><code><span class="k">for</span><span class="w"> </span><span class="o">((</span><span class="nv">i</span><span class="o">=</span><span class="m">1</span><span class="p">;</span><span class="m">10</span>&lt;<span class="o">=</span><span class="m">100</span><span class="p">;</span><span class="o">))</span><span class="p">;</span><span class="w"> </span><span class="k">do</span><span class="w"> </span><span class="se">\</span>
curl<span class="w"> </span>-H<span class="w"> </span><span class="s2">&quot;Connection: close&quot;</span><span class="w"> </span><span class="s2">&quot;http://node-app-nsX.apps.XXXXXXX.cloud.techzone.ibm.com&quot;</span><span class="w"> </span>--connect-timeout<span class="w"> </span><span class="m">1</span><span class="p">;</span><span class="w"> </span><span class="se">\</span>
<span class="k">done</span>
</code></pre></div>
<p><img alt="5.4.03-2" src="images/app-mod-ocp/5.4.03-2.png" style="max-height:700px" /></p>
<p>This load will increase the CPU usage and the Horizontal Pod Autoscaler will increase the number of instances. As soon as a new replica is started the OpenShift service object will redirect traffic to it so in the terminal where you executed the curl you will see first errors and the same HTML returned by the server because the application has not been started yet.</p>
<p>To avoid this behavior in OpenShift it exists also the parameter <strong>readinessProbe</strong> that works as the <strong>livenessProbe</strong> but it is used by OpenShift to add or remove the endpoint from the service object so if it does not return HTTP 200 the endpoint is not added to the service and the load is not balanced, this way you avoid to direct traffic to a new instance until the app is not ready to receive it. </p>
<p>You can check the deployment has autoscaled. </p>
<p><img alt="5.4.04" src="images/app-mod-ocp/5.4.04.png" style="max-height:700px" /></p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>You may need to open another terminal window and execute the endless for loop to give your application extra load to reach that 40% CPU usage set in the horizontal pod autoscaler</p>
</div>
<p>We can check current values from web console for example. Go from left menu panel to <strong>Workloads</strong> section and select <strong>Horizontal Pod Autoscalers</strong>. Then, click in our recently created policy and check the values.</p>
<p>Give it a couple of minutes and pods running should escalate to <strong>2</strong>. We can check the story of the auto-scaler in its <strong>Events</strong> tab:</p>
<p><img alt="5.4.05" src="images/app-mod-ocp/5.4.05.png" style="max-height:700px" /></p>
<p>You could also use:</p>
<div class="copy highlight"><span class="filename">Command</span><pre><span></span><code>oc describe hpa node-hpa -n nsX
</code></pre></div>
<p><div class="no-copy output highlight"><span class="filename">Output</span><pre><span></span><code>Name:                                                  node-hpa
Namespace:                                             ns30
Labels:                                                &lt;none&gt;
Annotations:                                           &lt;none&gt;
CreationTimestamp:                                     Wed, 27 Mar 2024 17:20:13 +0100
Reference:                                             Deployment/node-app
Metrics:                                               ( current / target )
  resource cpu on pods  (as a percentage of request):  12% (6m) / 40%
Min replicas:                                          1
Max replicas:                                          3
Deployment pods:                                       2 current / 2 desired
Conditions:
  Type            Status  Reason               Message
  ----            ------  ------               -------
  AbleToScale     True    ScaleDownStabilized  recent recommendations were higher than current one, applying the highest recent recommendation
  ScalingActive   True    ValidMetricFound     the HPA was able to successfully calculate a replica count from cpu resource utilization (percentage of request)
  ScalingLimited  False   DesiredWithinRange   the desired count is within the acceptable range
Events:
  Type     Reason                        Age                 From                       Message
  ----     ------                        ----                ----                       -------
  Warning  FailedComputeMetricsReplicas  12m (x4 over 38m)   horizontal-pod-autoscaler  invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: unable to get metrics for resource cpu: no metrics returned from resource metrics API
  Warning  FailedGetResourceMetric       12m (x5 over 38m)   horizontal-pod-autoscaler  failed to get cpu utilization: unable to get metrics for resource cpu: no metrics returned from resource metrics API
<span class="hll">  Normal   SuccessfulRescale             2m14s               horizontal-pod-autoscaler  New size: 2; reason: cpu resource utilization (percentage of request) above target
</span></code></pre></div>
Check the number of running pods:</p>
<div class="copy highlight"><span class="filename">Command</span><pre><span></span><code>oc get pods -n nsX
</code></pre></div>
<div class="no-copy output highlight"><span class="filename">Output</span><pre><span></span><code>NAME                        READY   STATUS    RESTARTS   AGE
node-app-7cfb58c968-7b5t6   1/1     Running   0          5m16s
node-app-7cfb58c968-9b24k   1/1     Running   0          16m
</code></pre></div>
<p>Stop the endless loop you have running on the other terminal window.</p>
<h3 id="5-clean-all-the-artifacts_2">5. Clean all the artifacts</h3>
<div class="copy highlight"><span class="filename">Command</span><pre><span></span><code>oc delete -f node-deployment-registry1.yaml
oc delete -f node-service1.yaml
oc delete -f node-ingress.yaml
oc delete -f node-hpa.yaml
</code></pre></div>
<h2 id="lab-6-network-policies">Lab 6 - Network Policies</h2>
<div class="admonition info">
<p class="admonition-title">Reference</p>
<p><a href="https://docs.openshift.com/container-platform/4.12/networking/network_policy/about-network-policy.html">https://docs.openshift.com/container-platform/4.12/networking/network_policy/about-network-policy.html</a></p>
</div>
<p>If you want to control traffic flow at the IP address or port level (OSI layer 3 or 4), then you might consider using OpenShift <strong>NetworkPolicies</strong> for a particular application in your cluster.</p>
<p>The entities that a Pod can communicate with are identified through a combination of the following 3 identifiers:</p>
<ol>
<li>Other pods that are allowed (exception: a pod cannot block access to itself)</li>
<li>Namespaces that are allowed.</li>
<li>IP blocks (exception: traffic to and from the node where a Pod is running is always allowed, regardless of the IP address of the Pod or the node)</li>
</ol>
<h3 id="1-create-two-deployments-and-create-a-route">1. Create two deployments and create a route</h3>
<p>For this lab, you must use a very basic sample application repository that can be built and deployed on OpenShift. The application repository contains Dockerfiles for Nginx images from the repository (<strong>Software Collections</strong>).</p>
<p><a href="https://github.com/sclorg/nginx-container">https://github.com/sclorg/nginx-container</a></p>
<p>Create two deployments using the <strong>oc new-app</strong> command (will be explained later on), name the first "front" and the second "backend":</p>
<div class="copy highlight"><span class="filename">Command</span><pre><span></span><code>oc new-app nginx~https://github.com/sclorg/nginx-ex --name=front
oc new-app nginx~https://github.com/sclorg/nginx-ex --name=backend
</code></pre></div>
<p>Create a route to the "front" service:</p>
<div class="copy highlight"><span class="filename">Command</span><pre><span></span><code>oc expose service front
</code></pre></div>
<h3 id="2-verify-access-to-pod">2. Verify access to pod</h3>
<p>Verify access to the "front" pod with the <code>oc rsh</code> and <code>curl</code> commands.</p>
<p>Get route information:</p>
<div class="copy highlight"><span class="filename">Command</span><pre><span></span><code>oc get route
</code></pre></div>
<div class="no-copy output highlight"><span class="filename">Output</span><pre><span></span><code>NAME    HOST/PORT                                                         PATH   SERVICES   PORT       TERMINATION   WILDCARD
front   front-ns30.apps.65f05030777e25001ee8fdfa.cloud.techzone.ibm.com          front      8080-tcp                 None
</code></pre></div>
<p>Get services information:</p>
<div class="copy highlight"><span class="filename">Command</span><pre><span></span><code>oc get services
</code></pre></div>
<div class="no-copy output highlight"><span class="filename">Output</span><pre><span></span><code>NAME      TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)             AGE
backend   ClusterIP   172.30.168.241   &lt;none&gt;        8080/TCP,8443/TCP   68s
front     ClusterIP   172.30.108.64    &lt;none&gt;        8080/TCP,8443/TCP   69s
</code></pre></div>
<p>Get pods information:</p>
<div class="copy highlight"><span class="filename">Command</span><pre><span></span><code>oc get pods
</code></pre></div>
<div class="no-copy output highlight"><span class="filename">Output</span><pre><span></span><code>NAME                       READY   STATUS      RESTARTS   AGE
backend-1-build            0/1     Completed   0          83s
backend-5985b9f6f9-qbgx9   1/1     Running     0          66s
front-1-build              0/1     Completed   0          84s
front-659fcd86d6-fh28x     1/1     Running     0          67s
</code></pre></div>
<p>Use the <code>oc rsh</code> and <code>curl</code> commands to confirm that the "backend" pod can access the IP address of the "front" pod.  </p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p><code>oc rsh</code> allows to enter inside the pod to test the connection. In the following command, replace <code>backend-5985b9f6f9-qbgx9</code> with the pod name from the previous command and <code>172.30.108.64</code> with the backend service IP from the output you obtained above</p>
</div>
<div class="copy highlight"><span class="filename">Command</span><pre><span></span><code>oc rsh backend-5985b9f6f9-qbgx9 curl 172.30.108.64:8080 | grep nginx
</code></pre></div>
<div class="no-copy output highlight"><span class="filename">Output</span><pre><span></span><code>    <span class="p">&lt;</span><span class="nt">h1</span><span class="p">&gt;</span>Welcome to your static nginx application on OpenShift<span class="p">&lt;/</span><span class="nt">h1</span><span class="p">&gt;</span>
</code></pre></div>
<p>Verify access to the hello pod using the curl command against the URL of the "front" route.</p>
<div class="copy highlight"><span class="filename">Command</span><pre><span></span><code>curl -s front-nsX.apps.XXXXXXX.cloud.techzone.ibm.com | grep nginx
</code></pre></div>
<div class="no-copy output highlight"><span class="filename">Output</span><pre><span></span><code>    <span class="p">&lt;</span><span class="nt">h1</span><span class="p">&gt;</span>Welcome to your static nginx application on OpenShift<span class="p">&lt;/</span><span class="nt">h1</span><span class="p">&gt;</span>
</code></pre></div>
<!-- !!! danger
    **REVIEW** **REVIEW** **REVIEW** **REVIEW** **REVIEW**

!!! info
    Following steps has been done by cluster admin: 

    <div class="no-copy highlight"><pre><span></span><code>oc label namespace default network.openshift.io/policy-group=ingress
</code></pre></div>

    Applying this label to the default namespace is only necessary because the classroom's default Ingress Controller uses the HostNetwork endpoint publishing strategy.

    <div class="no-copy highlight"><pre><span></span><code>oc project ns-network
oc new-app nginx~https://github.com/sclorg/nginx-ex --name=test
oc label namespace ns-network name=ns-network
</code></pre></div>

!!! danger
    **REVIEW** **REVIEW** **REVIEW** **REVIEW** **REVIEW** -->

<p>Change to project ns-network and get the pods running with "test" application:</p>
<div class="copy highlight"><span class="filename">Command</span><pre><span></span><code>oc project ns-network
oc get pods
</code></pre></div>
<div class="no-copy output highlight"><span class="filename">Output</span><pre><span></span><code>Now using project &quot;ns-network&quot; on server &quot;https://api.65f05030777e25001ee8fdfa.cloud.techzone.ibm.com:6443&quot;.
NAME                   READY   STATUS      RESTARTS   AGE
test-1-build           0/1     Completed   0          7h21m
test-f47c85c89-x8j2f   1/1     Running     0          7h21m
</code></pre></div>
<p>Verify that pods can access the "front" and "backend" pods in the <strong>ns-network</strong> namespace.</p>
<p>Use the <code>oc rsh</code> and <code>curl</code> commands to confirm that the "test" pod can access the IP address of the "front" and "backend" services.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Replace <code>test-f47c85c89-x8j2f</code> pod name with your pod name and the IPs with the IPs you collected from previous steps for the frontend and backend services.</p>
</div>
<div class="copy highlight"><span class="filename">Command</span><pre><span></span><code>oc rsh test-f47c85c89-x8j2f curl 172.30.168.241:8080 | grep nginx
oc rsh test-f47c85c89-x8j2f curl 172.30.108.64:8080 | grep nginx
</code></pre></div>
<div class="no-copy output highlight"><span class="filename">Output</span><pre><span></span><code>    <span class="p">&lt;</span><span class="nt">h1</span><span class="p">&gt;</span>Welcome to your static nginx application on OpenShift<span class="p">&lt;/</span><span class="nt">h1</span><span class="p">&gt;</span>
</code></pre></div>
<h3 id="3-create-the-deny-all-network-policy">3. Create the deny-all network policy</h3>
<p>From the nsX project, create the <strong>deny-all</strong> network policy:</p>
<div class="copy highlight"><span class="filename">Command</span><pre><span></span><code>oc project nsX
</code></pre></div>
<p>Create a file called <strong>deny-all.yaml</strong> with this content:</p>
<div class="copy highlight"><span class="filename">Command</span><pre><span></span><code><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">NetworkPolicy</span>
<span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">networking.k8s.io/v1</span>
<span class="nt">metadata</span><span class="p">:</span>
<span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">deny-all</span>
<span class="nt">spec</span><span class="p">:</span>
<span class="w">  </span><span class="nt">podSelector</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">{}</span>
</code></pre></div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>The indentation spaces are very important. Pay attention when executing the copy and paste. If you have problems with the copy and paste, all the files used in the labs are in the artifact folder under the user home folder.</p>
</div>
<p>Once you have the file created, execute:</p>
<div class="copy highlight"><span class="filename">Command</span><pre><span></span><code>oc create -f deny-all.yaml
</code></pre></div>
<p>Verify there is no longer access to the pods in the nsX namespace to the "front" pod via the exposed route. Press <code>Ctrl+C</code> to exit the curl command that does not reply:</p>
<div class="copy highlight"><span class="filename">Command</span><pre><span></span><code>curl -s -v front-nsX.apps.XXXXXXX.cloud.techzone.ibm.com | grep nginx
</code></pre></div>
<div class="no-copy output highlight"><span class="filename">Output</span><pre><span></span><code>*   Trying 169.50.187.164:80...
* Connected to front-ns30.apps.65f05030777e25001ee8fdfa.cloud.techzone.ibm.com (169.50.187.164) port 80
&gt; GET / HTTP/1.1
&gt; Host: front-ns30.apps.65f05030777e25001ee8fdfa.cloud.techzone.ibm.com
&gt; User-Agent: curl/8.4.0
&gt; Accept: */*
&gt; 
* HTTP 1.0, assume close after body
<span class="hll">&lt; HTTP/1.0 503 Service Unavailable
</span>&lt; pragma: no-cache
&lt; cache-control: private, max-age=0, no-cache, no-store
&lt; content-type: text/html
&lt; 
{ [2503 bytes data]
* Closing connection
</code></pre></div>
<div class="admonition warning">
<p class="admonition-title">Important</p>
<p>If the pod lands on the same node as a <strong>router-default</strong> pod, then the curl command works when traffic goes through that router pod. This is only the case with three-node clusters. In a traditional OpenShift cluster, where the control plane or infrastructure nodes are separated from the compute nodes, the network policy would apply to all the router pods in the cluster.</p>
</div>
<p>Verify that the "backend" pod can no longer access the IP address of the "front" pod. Wait a few seconds, and then press <code>Ctrl+C</code> to exit the curl command that does not reply:</p>
<div class="copy highlight"><span class="filename">Command</span><pre><span></span><code>oc get pods
</code></pre></div>
<div class="no-copy output highlight"><span class="filename">Output</span><pre><span></span><code>NAME                       READY   STATUS      RESTARTS   AGE
backend-1-build            0/1     Completed   0          10m
backend-5985b9f6f9-qbgx9   1/1     Running     0          9m45s
front-1-build              0/1     Completed   0          10m
front-659fcd86d6-fh28x     1/1     Running     0          9m46s
</code></pre></div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Again, change the pod name and IP to your corresponding values</p>
</div>
<div class="copy highlight"><span class="filename">Command</span><pre><span></span><code>oc rsh backend-5985b9f6f9-qbgx9 curl 172.30.108.64:8080 | grep nginx
</code></pre></div>
<p>Press <code>Ctrl+c</code> after some time when you see the curl command does not return.</p>
<h3 id="4-create-a-network-policy-to-allow-traffic">4. Create a network policy to allow traffic</h3>
<p>Create a network policy to allow traffic to the "front" pod in the nsX namespace from the test pod in the ns-network namespace over TCP on port 8080.</p>
<p>Create a file called <strong>allow-specific.yaml</strong> with this content:</p>
<div class="copy highlight"><span class="filename">Command</span><pre><span></span><code><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">NetworkPolicy</span>
<span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">networking.k8s.io/v1</span>
<span class="nt">metadata</span><span class="p">:</span>
<span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">allow-specific</span>
<span class="nt">spec</span><span class="p">:</span>
<span class="w">  </span><span class="nt">podSelector</span><span class="p">:</span>
<span class="w">    </span><span class="nt">matchLabels</span><span class="p">:</span>
<span class="w">      </span><span class="nt">deployment</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">front</span>
<span class="w">  </span><span class="nt">ingress</span><span class="p">:</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">from</span><span class="p">:</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">namespaceSelector</span><span class="p">:</span>
<span class="w">          </span><span class="nt">matchLabels</span><span class="p">:</span>
<span class="w">            </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ns-network</span>
<span class="w">        </span><span class="nt">podSelector</span><span class="p">:</span>
<span class="w">          </span><span class="nt">matchLabels</span><span class="p">:</span>
<span class="w">            </span><span class="nt">deployment</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">test</span>
<span class="w">      </span><span class="nt">ports</span><span class="p">:</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">port</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">8080</span>
<span class="w">        </span><span class="nt">protocol</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">TCP</span>
</code></pre></div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>The indentation spaces are very important. Pay attention when executing the copy and paste. If you have problems with the copy and paste, all the files used in the labs are in the artifact folder under the user home folder.</p>
</div>
<p>Once you have the file created, execute:</p>
<div class="copy highlight"><span class="filename">Command</span><pre><span></span><code>oc create -f allow-specific.yaml -n nsX
</code></pre></div>
<div class="no-copy output highlight"><span class="filename">Output</span><pre><span></span><code>networkpolicy.networking.k8s.io/allow-specific created
</code></pre></div>
<p>View the network policies in the nsX namespace:</p>
<div class="copy highlight"><span class="filename">Command</span><pre><span></span><code>oc get networkpolicies -n nsX
</code></pre></div>
<div class="no-copy output highlight"><span class="filename">Output</span><pre><span></span><code>NAME             POD-SELECTOR       AGE
allow-specific   deployment=front   20s
deny-all         &lt;none&gt;             4m59s
</code></pre></div>
<p>The <strong>allow-specific</strong> network policy uses labels to match the name of a namespace. By default, namespaces and projects do not get any labels automatically.</p>
<p>Confirm the label was applied on the ns-network project:</p>
<div class="copy highlight"><span class="filename">Command</span><pre><span></span><code>oc project ns-network
oc describe namespace ns-network
</code></pre></div>
<div class="no-copy output highlight"><span class="filename">Output</span><pre><span></span><code>Now using project &quot;ns-network&quot; on server &quot;https://api.65f05030777e25001ee8fdfa.cloud.techzone.ibm.com:6443&quot;.
Name:         ns-network
Labels:       kubernetes.io/metadata.name=ns-network
<span class="hll">              name=ns-network
</span>              pod-security.kubernetes.io/audit=restricted
              pod-security.kubernetes.io/audit-version=v1.24
              pod-security.kubernetes.io/warn=restricted
              pod-security.kubernetes.io/warn-version=v1.24
Annotations:  openshift.io/description: 
              openshift.io/display-name: 
              openshift.io/requester: kube:admin
              openshift.io/sa.scc.mcs: s0:c28,c17
              openshift.io/sa.scc.supplemental-groups: 1000790000/10000
              openshift.io/sa.scc.uid-range: 1000790000/10000
Status:       Active

No resource quota.

No LimitRange resource.
</code></pre></div>
<p>Verify that the test pod can access the IP address of the front pod or service, but cannot access the IP address of the backend pod or service.</p>
<div class="copy highlight"><span class="filename">Command</span><pre><span></span><code>oc get pods -n nsX -o wide
</code></pre></div>
<div class="no-copy output highlight"><span class="filename">Output</span><pre><span></span><code>NAME                       READY   STATUS      RESTARTS   AGE   IP             NODE       NOMINATED NODE   READINESS GATES
backend-1-build            0/1     Completed   0          14m   10.131.0.159   worker-2   &lt;none&gt;           &lt;none&gt;
backend-5985b9f6f9-qbgx9   1/1     Running     0          14m   10.131.0.160   worker-2   &lt;none&gt;           &lt;none&gt;
front-1-build              0/1     Completed   0          14m   10.131.0.158   worker-2   &lt;none&gt;           &lt;none&gt;
front-659fcd86d6-fh28x     1/1     Running     0          14m   10.128.2.103   worker-4   &lt;none&gt;           &lt;none&gt;
</code></pre></div>
<div class="copy highlight"><span class="filename">Command</span><pre><span></span><code>oc get pods
</code></pre></div>
<div class="no-copy output highlight"><span class="filename">Output</span><pre><span></span><code>NAME                   READY   STATUS      RESTARTS   AGE
test-1-build           0/1     Completed   0          7h31m
test-f47c85c89-x8j2f   1/1     Running     0          7h30m
</code></pre></div>
<p>Verify access to the front pod in the ns-network namespace:</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Again, change the pod name and IP to your corresponding values</p>
</div>
<div class="copy highlight"><span class="filename">Command</span><pre><span></span><code>oc rsh test-f47c85c89-x8j2f curl 10.128.2.103:8080 | grep nginx
</code></pre></div>
<p>Verify there is no access to the backend pod. Wait a few seconds, and then press <code>Ctrl+C</code> to exit the curl command that does not reply:</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Again, change the pod name and IP to your corresponding values</p>
</div>
<div class="copy highlight"><span class="filename">Command</span><pre><span></span><code>oc rsh test-f47c85c89-x8j2f curl 10.131.0.160:8080 | grep nginx
</code></pre></div>
<div class="no-copy output highlight"><span class="filename">Output</span><pre><span></span><code>oc rsh test-f47c85c89-x8j2f curl 10.128.2.103:8080 | grep nginx
    &lt;h1&gt;Welcome to your static nginx application on OpenShift&lt;/h1&gt;

oc rsh test-f47c85c89-x8j2f curl 10.131.0.160:8080 | grep nginx
command terminated with exit code 130
</code></pre></div>
<p>Create a network policy to allow traffic to the front pod from the exposed route. Use the resource file available at <strong>artifacts/allow-from-openshift-ingress.yaml</strong>:</p>
<div class="copy highlight"><span class="filename">Command</span><pre><span></span><code><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">NetworkPolicy</span>
<span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">networking.k8s.io/v1</span>
<span class="nt">metadata</span><span class="p">:</span>
<span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">allow-from-openshift-ingress</span>
<span class="nt">spec</span><span class="p">:</span>
<span class="w">  </span><span class="nt">podSelector</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">{}</span>
<span class="w">  </span><span class="nt">ingress</span><span class="p">:</span>
<span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">from</span><span class="p">:</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">namespaceSelector</span><span class="p">:</span>
<span class="w">        </span><span class="nt">matchLabels</span><span class="p">:</span>
<span class="w">          </span><span class="nt">network.openshift.io/policy-group</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ingress</span>
</code></pre></div>
<p>Change to your project nsX and create the policy:</p>
<div class="copy highlight"><span class="filename">Command</span><pre><span></span><code>oc project nsX
oc create -f allow-from-openshift-ingress.yaml -n nsX
</code></pre></div>
<div class="no-copy output highlight"><span class="filename">Output</span><pre><span></span><code>networkpolicy.networking.k8s.io/allow-from-openshift-ingress created
</code></pre></div>
<p>Verify access to the front pod via the exposed route:</p>
<div class="copy highlight"><span class="filename">Command</span><pre><span></span><code>curl front-nsX.apps.XXXXXXX.cloud.techzone.ibm.com | grep nginx
</code></pre></div>
<div class="no-copy output highlight"><span class="filename">Output</span><pre><span></span><code>    <span class="p">&lt;</span><span class="nt">h1</span><span class="p">&gt;</span>Welcome to your static nginx application on OpenShift<span class="p">&lt;/</span><span class="nt">h1</span><span class="p">&gt;</span>
</code></pre></div>
<p>From a web browser type <a href="http://front-nsX.apps.XXXXXXX.cloud.techzone.ibm.com">http://front-nsX.apps.XXXXXXX.cloud.techzone.ibm.com</a>:</p>
<p><img alt="6.4.01" src="images/app-mod-ocp/6.4.01.png" style="max-height:500px" /></p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Bear in mind it is <strong>HTTP</strong> protocol</p>
</div>
<h3 id="5-clean-all-the-artifacts_3">5. Clean all the artifacts</h3>
<div class="copy highlight"><span class="filename">Command</span><pre><span></span><code>oc project nsX
oc delete all -l app=front
oc delete all -l app=backend
oc delete route front
</code></pre></div>
<h2 id="lab-7-operators">Lab 7 - Operators</h2>
<div class="admonition info">
<p class="admonition-title">Reference</p>
<p><a href="https://docs.openshift.com/container-platform/4.12/operators/understanding/olm-what-operators-are.html">https://docs.openshift.com/container-platform/4.12/operators/understanding/olm-what-operators-are.html</a></p>
</div>
<p>Operators are software extensions to OpenShift that make use of custom resources to manage applications and their components.</p>
<p>The Operator pattern aims to capture the key aim of a human operator who is managing a service or set of services. Human operators who look after specific applications and services have deep knowledge of how the system ought to behave, how to deploy it, and how to react if there are problems.</p>
<p>People who run workloads on OpenShift often like to use automation to take care of repeatable tasks. The Operator pattern captures how you can write code to automate a task beyond what OpenShift itself provides.</p>
<p>Demystifying the operators, all the automation must be implemented by the operator provider, so there is not out of the box upgrade, backup, restore or other features just because something is an operator, those features must be coded in the operator by the operator provider. The automation available out of the box is the native automation of OpenShift for things like health check, scalability, load balancing and so on. An operator is a container developed by a software provider running in OpenShift looking to the state of "proprietary/custom objects (CRDs)" and taking actions depending on the values of that object.</p>
<p>Based on the features offered by the operators they are catalogued in these groups:</p>
<p><img alt="7.0.01" src="images/app-mod-ocp/7.0.01.png" style="max-height:250px" /></p>
<p>OpenShift supports and install out of the box the Operator LifeCycle Manager (OLM), OLM provides a set of features to help in the installation and management of Operators.</p>
<p><a href="https://github.com/operator-framework/operator-lifecycle-manager">https://github.com/operator-framework/operator-lifecycle-manager</a></p>
<p>"OLM extends OpenShift to provide a declarative way to install, manage, and upgrade Operators and their dependencies in a cluster".</p>
<h3 id="1-deploy-an-operator-using-the-operators-catalogues">1. Deploy an operator using the Operators Catalogues</h3>
<p>By default, there are four operator catalogues:</p>
<ul>
<li>Red Hat Operators: Red Hat products packaged and shipped by Red Hat. Supported by Red Hat.</li>
<li>Certified Operators: Products from leading independent software vendors (ISVs). Red Hat partners with ISVs to package and ship. Supported by the ISV.</li>
<li>Community Operators: Optionally visible software maintained by relevant representatives in the operator-framework/community-operators GitHub repository. No official support. Access to <a href="https://github.com/operator-framework/community-operators/tree/master/community-operators">https://github.com/operator-framework/community-operators/tree/master/community-operators</a> to view all available operators.</li>
<li>Custom Operators: Operators you add to your own quay.io app repository. If you have not added any Custom Operators, the Custom category does not appear in the web console on your OperatorHub.</li>
</ul>
<p>The OpenShift Operator Hub UI console is a feature offered by OpenShift that provides a user interface to deploy operators. The deployment of operators can be done by cluster admin roles using UI or CLI.</p>
<p>For this Lab, we have given the rights to all users in order to install the operators on each namespace. </p>
<p>This is how the we can deploy a Kafka operator:</p>
<p><img alt="7.1.01" src="images/app-mod-ocp/7.1.01.png" style="max-height:900px" /></p>
<div class="admonition info">
<p class="admonition-title">Details of this operator</p>
<ul>
<li><a href="https://operatorhub.io/operator/strimzi-kafka-operator">https://operatorhub.io/operator/strimzi-kafka-operator</a> and </li>
<li><a href="https://strimzi.io/docs/quickstart/latest/">https://strimzi.io/docs/quickstart/latest/</a></li>
</ul>
</div>
<p>Click on <strong>Continue</strong> on the message warning.</p>
<p><img alt="7.1.02" src="images/app-mod-ocp/7.1.02.png" style="max-height:400px" /></p>
<p><img alt="7.1.03" src="images/app-mod-ocp/7.1.03.png" style="max-height:800px" /></p>
<p><img alt="7.1.04" src="images/app-mod-ocp/7.1.04.png" style="max-height:900px" /></p>
<p>Here we are configuring three things:</p>
<ul>
<li>The namespace or group of namespaces where the operator will listen for operator instances or CRDs. In our case <strong>nsX</strong>.</li>
<li>The Channel to listen to updates for the operator. By default is the "<strong>Stable</strong>" Channel.</li>
<li>The approval strategy, if the update of the operator is done automatically or need manual approval through the console or API.</li>
</ul>
<p>This action creates two objects in OpenShift, these two objects are managed by the OLM, an <strong>Operator Group</strong> and a <strong>Subscription</strong>. The operator group is used mainly to handle security configuration in the namespaces selected for the operator and it stores also what namespaces the operator is watching for CRDs and the subscription is the way to specify what version of the operator to install and how to update it. The link between the Operator Group and the Subscription is that both are in the same namespace.</p>
<p><img alt="7.1.05" src="images/app-mod-ocp/7.1.05.png" style="max-height:300px" /></p>
<p>Execute this command to see the created operator:</p>
<div class="copy highlight"><span class="filename">Command</span><pre><span></span><code>oc get operatorgroup -n nsX
</code></pre></div>
<div class="no-copy output highlight"><span class="filename">Output</span><pre><span></span><code>NAME         AGE
ns30-9qxg5   28s
</code></pre></div>
<div class="copy highlight"><span class="filename">Command</span><pre><span></span><code>oc get operatorgroup ns30-9qxg5 -o yaml -n nsX
</code></pre></div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Change <code>ns30-9qxg5</code> for your operator group name retrieved on the previous command</p>
</div>
<p>You should see something like this:</p>
<div class="no-copy highlight"><pre><span></span><code><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">operators.coreos.com/v1</span>
<span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">OperatorGroup</span>
<span class="nt">metadata</span><span class="p">:</span>
<span class="hll"><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">nsX-9qxg5</span>
</span><span class="hll"><span class="w">  </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ns30</span>
</span><span class="w">  </span><span class="l l-Scalar l-Scalar-Plain">...</span>
<span class="nt">spec</span><span class="p">:</span>
<span class="w">  </span><span class="nt">targetNamespaces</span><span class="p">:</span>
<span class="hll"><span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ns30</span>
</span></code></pre></div>
<p>Then execute:</p>
<p><div class="copy highlight"><span class="filename">Command</span><pre><span></span><code>oc get subscriptions.operators.coreos.com strimzi-kafka-operator -o yaml -n nsX
</code></pre></div>
 
You should see something like this:</p>
<div class="no-copy highlight"><pre><span></span><code><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">operators.coreos.com/v1alpha1</span>
<span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Subscription</span>
<span class="nt">metadata</span><span class="p">:</span>
<span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">strimzi-kafka-operator</span>
<span class="hll"><span class="w">  </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ns30</span>
</span><span class="w">  </span><span class="l l-Scalar l-Scalar-Plain">...</span>
<span class="nt">spec</span><span class="p">:</span>
<span class="w">  </span><span class="nt">channel</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">stable</span>
<span class="w">  </span><span class="nt">installPlanApproval</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Automatic</span>
<span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">strimzi-kafka-operator</span>
<span class="w">  </span><span class="nt">source</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">community-operators</span>
<span class="w">  </span><span class="nt">sourceNamespace</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">openshift-marketplace</span>
<span class="w">  </span><span class="nt">startingCSV</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">strimzi-cluster-operator.v0.40.0</span>
<span class="w">  </span><span class="l l-Scalar l-Scalar-Plain">...</span>
</code></pre></div>
<p>With these two objects, OLM is executing all the steps documented in this URL: <a href="https://strimzi.io/docs/quickstart/latest/#proc-install-product-str">https://strimzi.io/docs/quickstart/latest/#proc-install-product-str</a></p>
<h3 id="2-deploy-a-service-instance-using-the-operator">2. Deploy a service instance using the operator</h3>
<p>Finally, you as user can use this operator that is listening for CRDs in your namespace. Go to <strong>Installed Operators</strong> in the OpenShift web console:</p>
<p><img alt="7.2.01" src="images/app-mod-ocp/7.2.01.png" style="max-height:700px" /></p>
<p>And create a new Kafka cluster using the CRD enabled by the Strimzi Operator (If you can't see, wait a couple of minutes due it takes time and refresh the webpage):</p>
<p><img alt="7.2.02" src="images/app-mod-ocp/7.2.02.png" style="max-height:1000px" /></p>
<p>Copy the next YAML (<strong>operator.yaml</strong>) in the console (you will first need to switch to yaml view at the top), this is the Kafka CRD monitored by our operator or change the default values as we have marked (use your namespace).</p>
<p>We have changed the replicas to <strong>1</strong> to reduce the number of pods and simplify the security.</p>
<div class="copy highlight"><pre><span></span><code><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">kafka.strimzi.io/v1beta2</span>
<span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Kafka</span>
<span class="nt">metadata</span><span class="p">:</span>
<span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">my-cluster</span>
<span class="hll"><span class="w">  </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">nsX</span>
</span><span class="nt">spec</span><span class="p">:</span>
<span class="w">  </span><span class="nt">kafka</span><span class="p">:</span>
<span class="w">    </span><span class="nt">config</span><span class="p">:</span>
<span class="hll"><span class="w">      </span><span class="nt">offsets.topic.replication.factor</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span>
</span><span class="hll"><span class="w">      </span><span class="nt">transaction.state.log.replication.factor</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span>
</span><span class="hll"><span class="w">      </span><span class="nt">transaction.state.log.min.isr</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span>
</span><span class="w">      </span><span class="nt">log.message.format.version</span><span class="p">:</span><span class="w"> </span><span class="s">&#39;3.0&#39;</span>
<span class="w">      </span><span class="nt">inter.broker.protocol.version</span><span class="p">:</span><span class="w"> </span><span class="s">&#39;3.0&#39;</span>
<span class="w">    </span><span class="nt">version</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">3.7.0</span>
<span class="w">    </span><span class="nt">storage</span><span class="p">:</span>
<span class="w">      </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ephemeral</span>
<span class="hll"><span class="w">    </span><span class="nt">replicas</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span>
</span><span class="w">    </span><span class="nt">listeners</span><span class="p">:</span><span class="w"> </span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">plain</span>
<span class="w">        </span><span class="nt">port</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">9092</span>
<span class="w">        </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">internal</span>
<span class="w">        </span><span class="nt">tls</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">tls</span>
<span class="w">        </span><span class="nt">port</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">9093</span>
<span class="w">        </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">internal</span>
<span class="w">        </span><span class="nt">tls</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
<span class="w">  </span><span class="nt">entityOperator</span><span class="p">:</span>
<span class="w">    </span><span class="nt">topicOperator</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">{}</span>
<span class="w">    </span><span class="nt">userOperator</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">{}</span>
<span class="w">  </span><span class="nt">zookeeper</span><span class="p">:</span>
<span class="w">    </span><span class="nt">storage</span><span class="p">:</span>
<span class="w">      </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ephemeral</span>
<span class="hll"><span class="w">    </span><span class="nt">replicas</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span>
</span></code></pre></div>
<p><img alt="7.2.03" src="images/app-mod-ocp/7.2.03.png" style="max-height:800px" /></p>
<p>Or create it with:</p>
<div class="copy highlight"><span class="filename">Command</span><pre><span></span><code>oc create -f operator.yaml
</code></pre></div>
<p>There are some parameters that Strimzi operator doesn’t inform and they can’t be displayed in the UI like the Status and Version (<a href="https://github.com/strimzi/strimzi-kafka-operator/issues/2671">https://github.com/strimzi/strimzi-kafka-operator/issues/2671</a>), but you can see them with CLI instead.</p>
<p>Execute this command to check the status:</p>
<div class="copy highlight"><span class="filename">Command</span><pre><span></span><code>oc wait kafka/my-cluster --for=condition=Ready --timeout=300s -n nsX
</code></pre></div>
<p>When it is ready, it will answer with:</p>
<div class="no-copy output highlight"><span class="filename">Output</span><pre><span></span><code>kafka.kafka.strimzi.io/my-cluster condition met
</code></pre></div>
<p>It takes some time; you can also take a look at the pods:</p>
<p><div class="copy highlight"><span class="filename">Command</span><pre><span></span><code>oc get pods -n nsX
</code></pre></div>
 
<div class="no-copy output highlight"><span class="filename">Output</span><pre><span></span><code>NAME                                                READY   STATUS    RESTARTS   AGE
my-cluster-entity-operator-8678969c-x8v7n           2/2     Running   0          42s
my-cluster-kafka-0                                  1/1     Running   0          66s
my-cluster-zookeeper-0                              1/1     Running   0          98s
strimzi-cluster-operator-v0.40.0-76cc96b44b-7kvj2   1/1     Running   0          17m
</code></pre></div></p>
<div class="copy highlight"><span class="filename">Command</span><pre><span></span><code>oc describe kafka my-cluster -n nsX
</code></pre></div>
<div class="no-copy highlight"><pre><span></span><code><span class="nt">Status</span><span class="p">:</span>
<span class="w">  </span><span class="nt">Cluster Id</span><span class="p">:</span><span class="w">  </span><span class="l l-Scalar l-Scalar-Plain">wSDGZ1ZyQtGhugzE_XIszw</span>
<span class="w">  </span><span class="nt">Conditions</span><span class="p">:</span>
<span class="w">    </span><span class="nt">Last Transition Time</span><span class="p">:</span><span class="w">  </span><span class="l l-Scalar l-Scalar-Plain">2021-09-30T18:19:09.795903Z</span>
<span class="w">    </span><span class="nt">Message</span><span class="p">:</span><span class="w">               </span><span class="l l-Scalar l-Scalar-Plain">A Kafka cluster with a single replica and ephemeral storage will lose topic messages after any restart or rolling update.</span>
<span class="w">    </span><span class="nt">Reason</span><span class="p">:</span><span class="w">                </span><span class="l l-Scalar l-Scalar-Plain">KafkaStorage</span>
<span class="w">    </span><span class="nt">Status</span><span class="p">:</span><span class="w">                </span><span class="l l-Scalar l-Scalar-Plain">True</span>
<span class="w">    </span><span class="nt">Type</span><span class="p">:</span><span class="w">                  </span><span class="l l-Scalar l-Scalar-Plain">Warning</span>
<span class="w">    </span><span class="nt">Last Transition Time</span><span class="p">:</span><span class="w">  </span><span class="l l-Scalar l-Scalar-Plain">2021-09-30T18:19:09.796033Z</span>
<span class="w">    </span><span class="nt">Message</span><span class="p">:</span><span class="w">               </span><span class="l l-Scalar l-Scalar-Plain">A ZooKeeper cluster with a single replica and ephemeral storage will be in a defective state after any restart or rolling update. It is recommended that a minimum of three replicas are used.</span>
<span class="w">    </span><span class="nt">Reason</span><span class="p">:</span><span class="w">                </span><span class="l l-Scalar l-Scalar-Plain">ZooKeeperStorage</span>
<span class="w">    </span><span class="nt">Status</span><span class="p">:</span><span class="w">                </span><span class="l l-Scalar l-Scalar-Plain">True</span>
<span class="w">    </span><span class="nt">Type</span><span class="p">:</span><span class="w">                  </span><span class="l l-Scalar l-Scalar-Plain">Warning</span>
<span class="w">    </span><span class="nt">Last Transition Time</span><span class="p">:</span><span class="w">  </span><span class="l l-Scalar l-Scalar-Plain">2021-09-30T18:20:10.718Z</span>
<span class="w">    </span><span class="nt">Status</span><span class="p">:</span><span class="w">                </span><span class="l l-Scalar l-Scalar-Plain">True</span>
<span class="w">    </span><span class="nt">Type</span><span class="p">:</span><span class="w">                  </span><span class="l l-Scalar l-Scalar-Plain">Ready</span>
</code></pre></div>
<p>With these steps you have already deployed a Kafka cluster, it has only one node because we configured it with "replicas: 1" but we could have generated a cluster of several nodes, and all the logic needed to configure the cluster is executed automatically by the operator. The operator is who creates and executes all the things needed to start and configure the Kafka cluster.</p>
<p>Now you can test the deployed Kafka instance. This command executes a pod using a prebuilt image with the necessary logic to interact with Kafka cluster. We are going to send several messages to kafka:</p>
<div class="copy highlight"><span class="filename">Command</span><pre><span></span><code>oc run kafka-producer -ti \
  --image=quay.io/strimzi/kafka:latest-kafka-3.7.0 \
  --rm=true \
  --restart=Never \
<span class="hll">  -- bin/kafka-console-producer.sh --broker-list my-cluster-kafka-bootstrap.nsX:9092 --topic my-topic
</span></code></pre></div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>You can ignore this message like:</p>
<div class="no-copy highlight"><pre><span></span><code>&quot;WARN [Producer clientId=console-producer] Error while fetching metadata with correlation id 3 : {my-topic=LEADER_NOT_AVAILABLE} (org.apache.kafka.clients.NetworkClient)&quot;
</code></pre></div>
</div>
<p>You will get an interactive console where you will be able to input few events to get published to kafka:</p>
<div class="no-copy highlight"><pre><span></span><code>If you don&#39;t see a command prompt, try pressing enter.
&gt;test1
&gt;test2
&gt;test3
^C
pod &quot;kafka-producer&quot; deleted
pod nsX/kafka-producer terminated (Error)
</code></pre></div>
<p>Then, in order to read events from kafka and check taht it works, execute:</p>
<div class="copy highlight"><span class="filename">Command</span><pre><span></span><code>oc run kafka-consumer -ti \
  --image=quay.io/strimzi/kafka:latest-kafka-3.7.0 \
  --rm=true \
  --restart=Never \
<span class="hll">  -- bin/kafka-console-consumer.sh --bootstrap-server my-cluster-kafka-bootstrap.nsX:9092 --topic my-topic --from-beginning
</span></code></pre></div>
<p>Wait some seconds and it will receive the messages, <code>Ctrl + C</code> to quit.</p>
<div class="no-copy highlight"><pre><span></span><code>If you don&#39;t see a command prompt, try pressing enter.
test1
test2
test3
^C
</code></pre></div>
<p>As easy as you created the Kafka cluster, you can edit it with <strong><em>oc edit kafka my-cluster -n nsX</em></strong>. The operator will take the changes and reconfigure automatically the cluster. You could also work with any of the other objects managed by the operator.</p>
<h3 id="3-clean-all-the-artifacts">3. Clean all the artifacts</h3>
<p>Remove the instance created:</p>
<p><img alt="7.3.01" src="images/app-mod-ocp/7.3.01.png" style="max-height:700px" /></p>
<p>And uninstall the operator:</p>
<p><img alt="7.3.02" src="images/app-mod-ocp/7.3.02.png" style="max-height:750px" /></p>
<h2 id="lab-8-source2image-s2i">Lab 8 - Source2Image (S2I)</h2>
<p>Source-to-Image (S2I) is a tool for building reproducible, Docker-formatted container images. It produces ready-to-run images by injecting application source (from a Git repository) into a container image and assembling a new image. The new image incorporates the base image (the builder) and built source and is ready to use with the buildah run command. S2I supports incremental builds, which re-use previously downloaded dependencies, previously built artifacts, etc.</p>
<h3 id="1-create-the-image-from-the-git-repository">1. Create the image from the Git repository</h3>
<p>From the Developer perspective, select <strong>+Add</strong> and it show different options to deploy an application (we have seen some of them on previous labs). Select in this case <strong>From Git</strong>:</p>
<p><img alt="8.1.01" src="images/app-mod-ocp/8.1.01.png" style="max-height:1000px" /></p>
<p>Use following <strong>Git Repo URL</strong> that stores the node.js code</p>
<p><a href="https://github.com/svennam92/node-s2i-openshift">https://github.com/svennam92/node-s2i-openshift</a></p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>You have to Click outside the text field in order to check the Git URL is valid. You can see the "Validated" message and green ACK. </p>
</div>
<p><img alt="8.1.02" src="images/app-mod-ocp/8.1.02.png" style="max-height:700px" /></p>
<p>You can observe the recommended <strong>Builder Image(s)</strong> detected. It has detected the git repo provided stores Node.js code.</p>
<p>Double check the name of the app and select the creation of a route automatically.</p>
<p><img alt="8.1.03" src="images/app-mod-ocp/8.1.03.png" style="max-height:800px" /></p>
<p>Click <strong>Create</strong> in order to start the process of installation.</p>
<p>You can see there is a new application on Topology. </p>
<p><img alt="8.1.04" src="images/app-mod-ocp/8.1.04.png" style="max-height:700px" /></p>
<p>It takes a while. You can see the application moves from one state to another.</p>
<p><img alt="8.1.05" src="images/app-mod-ocp/8.1.05.png" style="max-height:500px" /></p>
<p>and the steps that are executed if you click on the middle of application.</p>
<p>When the application be ready deployed, it will appear on green:</p>
<p><img alt="8.1.06" src="images/app-mod-ocp/8.1.06.png" style="max-height:900px" /></p>
<p>A route has been generated for you. Click on it in order to access the new application. It will open a new tab.</p>
<p><a href="https://node-s2i-openshift-nsX.apps.XXXXXX.cloud.techzone.ibm.com/login.html">https://node-s2i-openshift-nsX.apps.XXXXXX.cloud.techzone.ibm.com/login.html</a></p>
<p>Enter a dummy user and password:</p>
<p><img alt="8.1.07" src="images/app-mod-ocp/8.1.07.png" style="max-height:700px" /></p>
<p><img alt="8.1.08" src="images/app-mod-ocp/8.1.08.png" style="max-height:700px" /></p>
<p>You can see <strong>build</strong> was done:</p>
<p><img alt="8.1.09" src="images/app-mod-ocp/8.1.09.png" style="max-height:500px" /></p>
<p>And the image was added such as <strong>ImageStream</strong>. Change to the <strong>Administrator</strong> perspective:</p>
<p><img alt="8.1.10" src="images/app-mod-ocp/8.1.10.png" style="max-height:700px" /></p>
<h3 id="2-clean-all-the-artifacts">2. Clean all the artifacts</h3>
<p>Remove the previous Image Streams:</p>
<p><img alt="8.2.01" src="images/app-mod-ocp/8.2.01.png" style="max-height:400px" /></p>
<p>Delete the deployment:</p>
<p><img alt="8.2.02" src="images/app-mod-ocp/8.2.02.png" style="max-height:500px" /></p>
<h2 id="lab-9-logging">Lab 9 - Logging</h2>
<p>By default, Docker and OpenShift can access to the logs that a container prints to its <strong>STDOUT</strong> and <strong>STDERR</strong>. There are also options to access to internal or custom containers logs.</p>
<h3 id="1-command-line">1. Command line</h3>
<p>List running pods and tail logs from one of them:</p>
<div class="copy highlight"><span class="filename">Command</span><pre><span></span><code>oc create -f was-deployment-registry1.yaml
oc create -f was-service1.yaml
oc create -f was-ingress.yaml
</code></pre></div>
<p>List the pods to check the application is running</p>
<div class="copy highlight"><span class="filename">Command</span><pre><span></span><code>oc get pods -n nsX
</code></pre></div>
<p>Now poke the application in order to produce some logging (run the command below several times)</p>
<div class="copy highlight"><span class="filename">Command</span><pre><span></span><code>curl http://websphere-liberty-app-nsX.apps.XXXXXXX.cloud.techzone.ibm.com/HelloK8s/health
</code></pre></div>
<p>Retrieve the logs for your application:</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Remember to use your pod name from the previous commands.</p>
</div>
<div class="copy highlight"><span class="filename">Command</span><pre><span></span><code>oc logs -f websphere-liberty-app-66cdc65d7-fwrmh -n nsX
</code></pre></div>
<div class="no-copy output highlight"><span class="filename">Output</span><pre><span></span><code>La, la, la...
Executing /health. Returning GREEN.
La, la, la...
Executing /health. Returning GREEN.
La, la, la...
Executing /health. Returning GREEN.
La, la, la...
La, la, la...
La, la, la...
La, la, la...
Executing /health. Returning GREEN.
La, la, la...
Executing /health. Returning GREEN.
La, la, la...
Executing /health. Returning GREEN.
La, la, la...
La, la, la...
</code></pre></div>
<h3 id="2-openshift-console">2. OpenShift Console</h3>
<p>Go in left menu panel to <strong>Workloads</strong> section and select <strong>Pods</strong>. Then click on the pod you are interested in and click on the <strong>Logs</strong> tab:</p>
<p><img alt="9.2.01" src="images/app-mod-ocp/9.2.01.png" style="max-height:500px" /></p>
<h3 id="3-kibana">3. Kibana</h3>
<p>OpenShift includes a logging stack based on <strong>EFK</strong>: <strong>ElasticSearch</strong>, <strong>Fluentd</strong> and <strong>Kibana</strong>. It's not installed by default as you can choose your own logging system.</p>
<div class="admonition info">
<p class="admonition-title">Info</p>
<p>Labs Administrators have installed the logging stack for you for this Lab.</p>
</div>
<p><img alt="9.3.01" src="images/app-mod-ocp/9.3.01.png" style="max-height:600px" /></p>
<p>In the OpenShift Container Platform console, click the <strong>Application Launcher</strong> and select <strong>Logging</strong>. A new browser tab will open requesting login (SSO is in the roadmap). Once you login, you will get Kibana homepage.</p>
<p><img alt="9.3.02" src="images/app-mod-ocp/9.3.02.png" style="max-height:400px" /></p>
<div class="admonition info">
<p class="admonition-title">Info</p>
<p>You may be asked to log into your OpenShift account again</p>
</div>
<p>A new browser tab will open requesting Authorize Access. Once you click on <strong>Allow selected Permissions</strong>, you will get to the Kibana homepage.</p>
<p><img alt="9.3.03" src="images/app-mod-ocp/9.3.03.png" style="max-height:350px" /></p>
<p>Create an index pattern:</p>
<p><img alt="9.3.04" src="images/app-mod-ocp/9.3.04.png" style="max-height:700px" /></p>
<p><img alt="9.3.05" src="images/app-mod-ocp/9.3.05.png" style="max-height:450px" /></p>
<p><img alt="9.3.06" src="images/app-mod-ocp/9.3.06.png" style="max-height:600px" /></p>
<p>Select your project (if you try to see others that are not assigned to you, you can't see it):</p>
<p><img alt="9.3.07" src="images/app-mod-ocp/9.3.07.png" style="max-height:400px" /></p>
<p>You can add the message field on the canvas:</p>
<p><img alt="9.3.08" src="images/app-mod-ocp/9.3.08.png" style="max-height:500px" /></p>
<p>If you don’t see the websphere-liberty-app logs, check the timeframe selected and click on <strong>Set to Now</strong>, in order to get the latest messages:</p>
<p><img alt="9.3.09" src="images/app-mod-ocp/9.3.09.png" style="max-height:500px" /></p>
<p>You can filter for container name (in case you have a lot of containers on your namespace):</p>
<p><img alt="9.3.10" src="images/app-mod-ocp/9.3.10.png" style="max-height:400px" /></p>
<p>You could delete the filter, selecting it and press the trash button:</p>
<p><img alt="9.3.11" src="images/app-mod-ocp/9.3.11.png" style="max-height:450px" /></p>
<p>Try to see another fake application (such as "kkk") and you will see there is nothing shown.</p>
<!-- ### 4. Using IBM Cloud Logging Service

As we are using OpenShift on IBM Cloud, we are able to use very easily the IBM Cloud Logging Service in order to centralize all the logs of your clusters and services.

You can access it via the **General Menu -> Observability**

![9.4.01](images/app-mod-ocp/9.4.01.png){: style="max-height:700px"}

Go to **Logging** and select **Open dashboard**. It'll open another tab:

![9.4.02](images/app-mod-ocp/9.4.02.png){: style="max-height:700px"}

![9.4.03](images/app-mod-ocp/9.4.03.png){: style="max-height:700px"}

Select the application on **Apps**, and click **Apply**:

![9.4.04](images/app-mod-ocp/9.4.04.png){: style="max-height:700px"}

You can click on one of the messages to see its details:

![9.4.05](images/app-mod-ocp/9.4.05.png){: style="max-height:700px"}

On the bottom you can filter by text (using pod name for example) to filter each application:

![9.4.06](images/app-mod-ocp/9.4.06.png){: style="max-height:700px"}

Or Click the Source identifier for the entry. Click the **+** icon and press enter in the search field:

![9.4.07](images/app-mod-ocp/9.4.07.png){: style="max-height:700px"} -->

<h2 id="lab-10-monitoring">Lab 10 - Monitoring</h2>
<p>OpenShift, out of the box, uses <strong>Prometheus</strong> and <strong>Grafana</strong> to monitor the state of your OpenShift cluster and the containers running on it.</p>
<div class="admonition info">
<p class="admonition-title">Reference</p>
<ul>
<li><a href="https://prometheus.io/">https://prometheus.io/</a></li>
<li><a href="https://grafana.com/">https://grafana.com/</a></li>
</ul>
</div>
<p>Basically, with Grafana you visualize the metrics collected in Prometheus.</p>
<p>In OpenShift, for users to have access to the monitoring of their namespaces, <a href="https://docs.openshift.com/container-platform/4.12/monitoring/enabling-monitoring-for-user-defined-projects.html#granting-users-permission-to-monitor-user-defined-projects_enabling-monitoring-for-user-defined-projects">access needs to be set up</a></p>
<p>As reference, this is the <strong>clusterrole</strong> configuration:</p>
<div class="copy highlight"><span class="filename">Command</span><pre><span></span><code>oc get clusterrole cluster-monitoring-view -o yaml
</code></pre></div>
<div class="no-copy highlight"><pre><span></span><code><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">rbac.authorization.k8s.io/v1</span>
<span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ClusterRole</span>
<span class="nt">metadata</span><span class="p">:</span>
<span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">cluster-monitoring-view</span>
<span class="nt">rules</span><span class="p">:</span>
<span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">apiGroups</span><span class="p">:</span>
<span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="s">&quot;&quot;</span>
<span class="w">  </span><span class="nt">resources</span><span class="p">:</span>
<span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">namespaces</span>
<span class="w">  </span><span class="nt">verbs</span><span class="p">:</span>
<span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">get</span>
</code></pre></div>
<p>Go to <strong>Monitoring</strong>:</p>
<p><img alt="10.0.01" src="images/app-mod-ocp/10.0.01.png" style="max-height:500px" /></p>
<p>In this section you have access to:</p>
<ul>
<li>Alerting: out of the box alerts configured against Prometheus metrics.</li>
<li>Metrics: Prometheus UI to query metrics.</li>
<li>Dashboards: Access to Grafana.</li>
</ul>
<h3 id="1-dashboards">1. Dashboards</h3>
<p><img alt="10.1.01" src="images/app-mod-ocp/10.1.01.png" style="max-height:1200px" /></p>
<p>There are some generic dashboards (you cannot modify them). You can filter using the top combo boxes:</p>
<p><img alt="10.1.02" src="images/app-mod-ocp/10.1.02.png" style="max-height:1200px" /></p>
<h3 id="2-alerts">2. Alerts</h3>
<div class="admonition info">
<p class="admonition-title">Reference</p>
<p><a href="https://docs.openshift.com/container-platform/4.12/monitoring/managing-alerts.html">https://docs.openshift.com/container-platform/4.12/monitoring/managing-alerts.html</a></p>
</div>
<p>The alerts are based on Prometheus metrics, the engine in charge of polling the alerts are the same that Prometheus.</p>
<p>In the OpenShift console click on <strong>Alerting</strong>.</p>
<p><img alt="10.2.01" src="images/app-mod-ocp/10.2.01.png" style="max-height:150px" /></p>
<p>The UI is provided by OpenShift. You can see the alerts configured and the state:</p>
<p><img alt="10.2.02" src="images/app-mod-ocp/10.2.02.png" style="max-height:600px" /></p>
<p>The defined alerts are in the configmap <strong>prometheus-k8s-rulefiles-0</strong> in the namespace <strong>openshift-monitoring</strong>.</p>
<p>In the YAML tab, it is possible to configure how the Prometheus alert engine behaves. This is useful for things like exporting the alerts to other systems compatible with Prometheus alerts.</p>
<p>This configuration can only be changed by administrator users:</p>
<p><img alt="10.2.03" src="images/app-mod-ocp/10.2.03.png" style="max-height:600px" /></p>
<h3 id="3-metrics">3. Metrics</h3>
<div class="admonition info">
<p class="admonition-title">Reference</p>
<p><a href="https://docs.openshift.com/container-platform/4.12/monitoring/managing-metrics.html">https://docs.openshift.com/container-platform/4.12/monitoring/managing-metrics.html</a></p>
</div>
<p>In the OpenShift console click on <strong>Metrics</strong>:</p>
<p><img alt="10.3.01" src="images/app-mod-ocp/10.3.01.png" style="max-height:200px" /></p>
<p>This is a UI to display Prometheus metrics values. For a sample query click on <strong>Insert Example Query</strong>:</p>
<p><img alt="10.3.02" src="images/app-mod-ocp/10.3.02.png" style="max-height:450px" /></p>
<p>It will display the values for the sample query:</p>
<p><img alt="10.3.03" src="images/app-mod-ocp/10.3.03.png" style="max-height:600px" /></p>
<p>Information about available metrics:</p>
<p><a href="https://github.com/kubernetes/kube-state-metrics/tree/master/docs#exposed-metrics">https://github.com/kubernetes/kube-state-metrics/tree/master/docs#exposed-metrics</a></p>
<!-- ### 4. Using IBM Cloud Monitoring Service

As we are using OpenShift on IBM Cloud we are going to use the IBM Cloud Monitoring Service in order to centralize all clusters and services monitoring.

You can access via the **General Menu -> Observability**

![10.4.01](images/app-mod-ocp/10.4.01.png){: style="max-height:700px"}

Go to **Monitoring** and select **Open dashboard**. It'll open another tab:

![10.4.02](images/app-mod-ocp/10.4.02.png){: style="max-height:700px"}

If you opened the Monitor immediately after it was created, you might see "No data available!" in many of the monitors. This is expected as the time interval is set to 1 hour be default and monitoring has only been running for a short period of time.

At the very bottom of the screen is the Time Navigation bar. This bar allows you to modify the time and date range of the current view. Modifying these parameters allows you to look back in time and get a closer look at a specific period of time.

Switch to view Host & Containers

![10.4.03](images/app-mod-ocp/10.4.03.png){: style="max-height:700px"}

Select on the filter, for example, **sysdig_host_cpu_used_percent**, to check the percentage used of CPU in each of the nodes forming the cluster. You can try other filters and values to see different visualizations of the service.

![10.4.04](images/app-mod-ocp/10.4.04.png){: style="max-height:700px"}

In the Dashboards tab, you can try different predefined dashboards. Check the **Cluster Capacity planning** dashboard to get an overview of the cluster capacity.

![10.4.05](images/app-mod-ocp/10.4.05.png){: style="max-height:700px"}

![10.4.06](images/app-mod-ocp/10.4.06.png){: style="max-height:700px"}

You can also check the dashboard **Pod Status & Performance**

![10.4.07](images/app-mod-ocp/10.4.07.png){: style="max-height:700px"} -->

<h2 id="lab-11-cicd-with-pipelines-tekton-and-argocd">Lab 11 - CI/CD with Pipelines Tekton and ArgoCD</h2>
<p>Let's review what is going to happen when running the CI/CD pipelines configured in the cluster.</p>
<h3 id="continuous-integration-ci">Continuous Integration (CI)</h3>
<p>On each push to the <strong>devops-ocp-app</strong> git repository on the Gitea server, the following steps are performed in the Pipeline:</p>
<ul>
<li>The webhook configured in gitea triggers the start of the pipeline;</li>
<li>The code is cloned from the Gitea server and the build is performed;</li>
<li>Unit tests can be run and in parallel the code is analyzed by SonarQube for anti-patterns, and a dependency report is generated;</li>
<li>The application is packaged as a WAR and released to the Sonatype Nexus artifact repository;</li>
<li>A container image is created in the DEV or STA environment, depending on the branch, pushed to the OpenShift internal registry and tagged with app-eap:latest;</li>
<li>Kubernetes manifests are updated in the Git config repository with the image digest that was created in the pipeline;</li>
<li>A pull-request is created in the config repository to merge the image summary update into the PROD environment, this step is only performed for the STA environment.</li>
</ul>
<p><img alt="11.1.01" src="images/app-mod-ocp/11.1.01.png" style="max-height:400px" /> </p>
<h3 id="continuous-delivery-cd">Continuous Delivery (CD)</h3>
<p>ArgoCD continuously monitors settings stored in the Git repository and uses Kustomize to override environment-specific settings when deploying the application to DEV, STA, and PROD environments.</p>
<p><img alt="11.1.02" src="images/app-mod-ocp/11.1.02.png" style="max-height:400px" /></p>
<h3 id="deploy-an-application-using-ci-pipelines-with-tekton">Deploy an Application using CI Pipelines with Tekton</h3>
<p>The objective is to create a pipeline responsible for deploying the application in the development and staging environments. For production, there will be a manual approval and it will be made available through a pull request in the GitOps configuration repository.</p>
<p><strong>Development pipeline</strong></p>
<p>It contains the main steps of a software continuous integration process.</p>
<p>A single Pipeline serves both development and staging environments. As we are using gitflow, the control of which environment will be executed into is in charge of the branch that starts the process -&gt; ex. branch develop for development and release for staging. For development, the Pull Request promotion task is not performed.</p>
<p>The next command clones the app repository from gitea server and creates a new branch called develop, then push to origin develop, so the webhook configured starts the pipeline for develop environment.</p>
<p>To put all of the above in motion follow the next steps:</p>
<p>Execute the commands below to clone the app repository, create a branch, make a change in a file and push the changes to the branch</p>
<ol>
<li>
<p>Clone the repository</p>
<div class="copy highlight"><span class="filename">Command</span><pre><span></span><code>git clone https://$(oc get route -n cicd gitea --template=&#39;{{.spec.host}}&#39;)/gitea/devops-ocp-app.git 
</code></pre></div>
</li>
<li>
<p>Enter in the directory where is the application code</p>
<div class="copy highlight"><span class="filename">Command</span><pre><span></span><code>cd devops-ocp-app
</code></pre></div>
</li>
<li>
<p>Set develop branch as current</p>
<div class="copy highlight"><span class="filename">Command</span><pre><span></span><code>git checkout -b develop
</code></pre></div>
</li>
<li>
<p>Get any new changes in the branch</p>
<div class="copy highlight"><span class="filename">Command</span><pre><span></span><code>git pull origin develop
</code></pre></div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>If you get an error after this command. Don't panic could it be the branch does not have anything to pull. Remember this repository is shared with others running the labs</p>
</div>
</li>
<li>
<p>Make a dummy change in a file</p>
<div class="copy highlight"><span class="filename">Command</span><pre><span></span><code>sed -i &#39;s/under the License/under the License dummy /g&#39; pom.xml
</code></pre></div>
</li>
<li>
<p>Add contents changed to local git</p>
<div class="copy highlight"><span class="filename">Command</span><pre><span></span><code>git add .
</code></pre></div>
</li>
<li>
<p>Configure your mail and name</p>
<div class="copy highlight"><span class="filename">Command</span><pre><span></span><code>git config --global user.email &quot;you@example.com&quot;
git config --global user.name &quot;Your name&quot;
</code></pre></div>
</li>
<li>
<p>Commit the changes to git</p>
<div class="copy highlight"><span class="filename">Command</span><pre><span></span><code>git commit -m &quot;Dummy comment&quot;
</code></pre></div>
</li>
<li>
<p>The next command will ask you for a user and password, where user and password are <code>gitea/devops</code> respectively</p>
<div class="copy highlight"><span class="filename">Command</span><pre><span></span><code>git push -u origin develop
</code></pre></div>
</li>
</ol>
<p>Check that a pipeline run is ongoing in the <strong>PipelineRuns</strong> tab: </p>
<p><img alt="11.1.03" src="images/app-mod-ocp/11.1.03.png" style="max-height:600px" /></p>
<p>Review all the tasks that are executed:</p>
<p><img alt="11.1.04" src="images/app-mod-ocp/11.1.04.png" style="max-height:700px" /></p>
<p>Review ArgoCD is in sync with the environment.</p>
<p>To get the ArgoCD Web Console URL execute this command:</p>
<div class="copy highlight"><span class="filename">Command</span><pre><span></span><code>oc get route -n cicd argocd-server --template=&#39;{{.spec.host}}&#39;
</code></pre></div>
<p>To login into ArgoCD, click on <strong>Log in via Openshift</strong> button on the ArgoCD welcome page</p>
<p><img alt="11.1.05" src="images/app-mod-ocp/11.1.05.png" style="max-height:600px" /></p>
<p>You can execute <strong>refresh</strong> or/and <strong>synchronize</strong> to keep ArgoCD in sync with the environment (the projects are enabled to be auto sync by default)</p>
<p><img alt="11.1.06" src="images/app-mod-ocp/11.1.06.png" style="max-height:600px" /></p>
<p>Review in gitea the repository that uses ArgoCD (<strong>devops-ocp-app-config</strong>) to understand the logic and how ArgoCD is configured from a git repository.</p>
<p>To get the gitea URL execute this command</p>
<p><div class="copy highlight"><span class="filename">Command</span><pre><span></span><code>oc get route -n cicd gitea --template=&#39;{{.spec.host}}&#39;
</code></pre></div>
Point your browser to the gitea URL and use <strong>gitea/devops</strong> as username and password respectively to log in.</p>
<p><img alt="11.1.07" src="images/app-mod-ocp/11.1.07.png" style="max-height:600px" /></p>
<p>Review the code scans in Sonarqube</p>
<p>To get the Sonarqube URL perform this command</p>
<div class="copy highlight"><span class="filename">Command</span><pre><span></span><code>oc get route -n cicd sonarqube --template=&#39;{{.spec.host}}&#39;
</code></pre></div>
<p>Point your browser to the Sonarqube URL and use <strong>admin/admin123</strong> as username and password respectively to log in.</p>
<p><img alt="11.1.08" src="images/app-mod-ocp/11.1.08.png" style="max-height:700px" /></p>
<p>Review the application is deployed in namespace <strong>dev</strong></p>
<p><img alt="11.1.09" src="images/app-mod-ocp/11.1.09.png" style="max-height:600px" /></p>
<p>Check the application route and click on it to access the application</p>
<p><img alt="11.1.10" src="images/app-mod-ocp/11.1.10.png" style="max-height:550px" /></p>
<p><img alt="11.1.11" src="images/app-mod-ocp/11.1.11.png" style="max-height:700px" /></p>
<h3 id="promote-an-application-using-cd-with-gitops-gitea-config-repo-and-argocd">Promote an Application using CD with GitOps (gitea config repo) and ArgoCD</h3>
<p>Execute the commands below to clone the app repository, create a branch (branch name is <strong>release</strong> for <strong>STA</strong> environment), make a change in a file and push the changes to the branch.</p>
<ol>
<li>
<p>Clone the repository</p>
<div class="copy highlight"><span class="filename">Command</span><pre><span></span><code>git clone https://$(oc get route -n cicd gitea --template=&#39;{{.spec.host}}&#39;)/gitea/devops-ocp-app.git 
</code></pre></div>
</li>
<li>
<p>Enter in the directory where is the application code</p>
<div class="copy highlight"><span class="filename">Command</span><pre><span></span><code>cd devops-ocp-app
</code></pre></div>
</li>
<li>
<p>Set develop branch as current</p>
<div class="copy highlight"><span class="filename">Command</span><pre><span></span><code>git checkout -b release
</code></pre></div>
</li>
<li>
<p>Get any new changes in the branch</p>
<div class="copy highlight"><span class="filename">Command</span><pre><span></span><code>git pull origin release
</code></pre></div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>If you get an error after this command. Don't panic could it be the branch does not have anything to pull. Remember this repository is shared with others running the labs</p>
</div>
</li>
<li>
<p>Make a dummy change in a file</p>
<div class="copy highlight"><span class="filename">Command</span><pre><span></span><code>sed -i &#39;s/under the License/under the License dummy /g&#39; pom.xml
</code></pre></div>
</li>
<li>
<p>Add contents changed to local git</p>
<div class="copy highlight"><span class="filename">Command</span><pre><span></span><code>git add .
</code></pre></div>
</li>
<li>
<p>Commit the changes to git</p>
<div class="copy highlight"><span class="filename">Command</span><pre><span></span><code>git commit -m &quot;Dummy comment&quot;
</code></pre></div>
</li>
<li>
<p>The next command will ask you for a user and password, where user and password are <code>gitea/devops</code> respectively</p>
<div class="copy highlight"><span class="filename">Command</span><pre><span></span><code>git push -u origin release
</code></pre></div>
</li>
</ol>
<p>Perform the same steps as in the previous chapter. Check the application is deployed in <strong>sta</strong> environment and test you can access it from openshift <strong>sta</strong> namespace.</p>
<p>The pipelines in <strong>sta</strong> perform an additional task <strong>promote-prod-pr</strong> to promote the change to production.</p>
<p><img alt="11.1.12" src="images/app-mod-ocp/11.1.12.png" style="max-height:800px" /></p>
<p>The task <strong>promote-prod-pr</strong> creates a pull request to the <strong>devops-ocp-app-config</strong> repository (the GitOps repository). The pull request contains the image digest of the last image available for <strong>PROD</strong>. </p>
<p>Please note that no image will be deployed in PROD by ArgoCD until the pull request is merged in the repository. </p>
<p>Review the pull request in gitea</p>
<p><img alt="11.1.13" src="images/app-mod-ocp/11.1.13.png" style="max-height:450px" /></p>
<p>Navigate to the contents of the commit associated with the pull request. Review the change to understand how ArgoCD will read the new image digest from the file modified by the <strong>sta</strong> pipeline (environments/prod/kustomization.yaml).</p>
<p><img alt="11.1.14" src="images/app-mod-ocp/11.1.14.png" style="max-height:600px" /></p>
<p>Merge the pull request changes to the main branch to make them available to ArgoCD</p>
<p><img alt="11.1.15" src="images/app-mod-ocp/11.1.15.png" style="max-height:600px" /></p>
<p>Check in ArgoCD and openshift namespace <strong>prod</strong> that a new image has been rolled out in <strong>prod</strong> for the application. </p>
<div class="admonition warning">
<p class="admonition-title">IMPORTANT</p>
<p>No new images will be deployed in production until someone accepts and merges the pull request as a precaution of not deploying in prod images not validated/approved.</p>
</div>
<h2 id="lab-12-application-modernization-with-transformation-advisor">Lab 12 - Application modernization with Transformation Advisor</h2>
<p>The steps to complete the lab are described in a video <a href="https://youtu.be/ss9joPm2M1U">here</a>. Instead, you can follow the steps below.</p>
<p>Open Transformation Advisor URL on your web browser. To get the url:</p>
<div class="copy highlight"><span class="filename">Command</span><pre><span></span><code>oc get route -n ta-project ta-ui-route --template=&#39;{{.spec.host}}&#39;
</code></pre></div>
<p><img alt="12.1.01" src="images/app-mod-ocp/12.1.01.png" style="max-height:500px" /></p>
<p>Click on <strong>Create new</strong> and give a name to the workspace: workspaceX</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Replace <strong>X</strong> with your <strong>ID</strong></p>
</div>
<p><img alt="12.1.02" src="images/app-mod-ocp/12.1.02.png" style="max-height:300px" /></p>
<div class="admonition warning">
<p class="admonition-title">Important</p>
<p>The following is just <strong>informational</strong> you do not have to do it in your lab environment as it has been already done for you.</p>
</div>
<p>Click on Download <strong>(do not perform this step in your lab just read)</strong></p>
<p><img alt="12.1.03" src="images/app-mod-ocp/12.1.03.png" style="max-height:600px" /></p>
<p>Download the data collector for your desktop architecture <strong>(do not perform this step in your lab just read)</strong></p>
<p><img alt="12.1.04" src="images/app-mod-ocp/12.1.04.png" style="max-height:650px" /></p>
<p>Install the data collector <strong>(do not perform this step in your lab just read)</strong></p>
<div class="copy highlight"><span class="filename">Command</span><pre><span></span><code>tar xvfz transformationadvisor-Linux_workspace01.tgz
cd transformationadvisor*
./bin/transformationadvisor --help
</code></pre></div>
<p>Run the data collector against your Websphere platform where you have the applications you want to migrate to containers <strong>(do not perform this step in your lab just read)</strong></p>
<div class="copy highlight"><span class="filename">Command</span><pre><span></span><code>./bin/transformationadvisor -w &lt;WEBSPHERE_HOME_DIR&gt; -p &lt;PROFILE_NAME&gt; [--scan-node --ignore-missing-binary --ignore-missing-shared-library --applications --applications-file] ([] denotes optional arguments)
</code></pre></div>
<p>As a result, you will have a zip file with the contents of the analysis. As we have not performed these steps in the lab please download an example from here <a href="assets/AppSrv01.zip">AppSrv01.zip</a></p>
<p>Upload the data collection downloaded into Transformation Advisor</p>
<p><img alt="12.1.05" src="images/app-mod-ocp/12.1.05.png" style="max-height:600px" /></p>
<p><img alt="12.1.06" src="images/app-mod-ocp/12.1.06.png" style="max-height:600px" /></p>
<p>You can set different targets of migration. Leave open liberty as the target migration.</p>
<p><img alt="12.1.07" src="images/app-mod-ocp/12.1.07.png" style="max-height:600px" /></p>
<p>Review the CustomerOrderServicesApp.ear analysis.</p>
<p><img alt="12.1.08" src="images/app-mod-ocp/12.1.08.png" style="max-height:600px" /></p>
<p>Open the detail of the analysis</p>
<p><img alt="12.1.09" src="images/app-mod-ocp/12.1.09.png" style="max-height:550px" /></p>
<p>Generate the Technology report.</p>
<p><img alt="12.1.10" src="images/app-mod-ocp/12.1.10.png" style="max-height:600px" /></p>
<p><img alt="12.1.11" src="images/app-mod-ocp/12.1.11.png" style="max-height:580px" /></p>
<p>Generate the Inventory report.</p>
<p><img alt="12.1.12" src="images/app-mod-ocp/12.1.12.png" style="max-height:600px" /></p>
<p><img alt="12.1.13" src="images/app-mod-ocp/12.1.13.png" style="max-height:600px" /></p>
<p>Generate the Migration Analysis Report.</p>
<p><img alt="12.1.14" src="images/app-mod-ocp/12.1.14.png" style="max-height:600px" /></p>
<p>Finally, click on <strong>View Migration plan</strong>.</p>
<p><img alt="12.1.15" src="images/app-mod-ocp/12.1.15.png" style="max-height:280px" /></p>
<p><img alt="12.1.16" src="images/app-mod-ocp/12.1.16.png" style="max-height:600px" /></p>
<p><strong>Download</strong> and <strong>review</strong> of the contents downloaded. The files will help on the migration of the application to containers.</p>
<h2 id="lab-13-scale-your-application-with-gitops-and-argocd">Lab 13 - Scale your application with GitOps and ArgoCD</h2>
<p>Using all you have learnt, demonstrate your skills with this advanced and minimally detailed laboratory.</p>
<p>Change the <strong>replicas</strong> of the application deployed in <strong>dev</strong> from <strong>3</strong> to <strong>2</strong>.</p>
<ol>
<li>Access gitea</li>
<li>Make a change to the <strong>kustomization.yaml</strong> that manages the number of replicas (you can edit the file directly using the gitea web interface)</li>
<li>Commit the change</li>
<li>Check in ArgoCD that the new replica count is read and applied in the namespace prod.</li>
<li>Look at openshift namespace prod to check the number of replicas deployed.</li>
</ol>
<hr />
<div class="admonition success">
<p class="admonition-title">Congratulations!</p>
<p>You have successfully completed the Application Modernization on Red Hat OpenShift Workshop. Well done!</p>
</div>
<p><img alt="approved" src="images/approved-wide.png" style="max-height:400px" /></p>







  
    
  
  


  <aside class="md-source-file">
    
      
  <span class="md-source-file__fact">
    <span class="md-icon" title="Last update">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M21 13.1c-.1 0-.3.1-.4.2l-1 1 2.1 2.1 1-1c.2-.2.2-.6 0-.8l-1.3-1.3c-.1-.1-.2-.2-.4-.2m-1.9 1.8-6.1 6V23h2.1l6.1-6.1-2.1-2M12.5 7v5.2l4 2.4-1 1L11 13V7h1.5M11 21.9c-5.1-.5-9-4.8-9-9.9C2 6.5 6.5 2 12 2c5.3 0 9.6 4.1 10 9.3-.3-.1-.6-.2-1-.2s-.7.1-1 .2C19.6 7.2 16.2 4 12 4c-4.4 0-8 3.6-8 8 0 4.1 3.1 7.5 7.1 7.9l-.1.2v1.8Z"/></svg>
    </span>
    <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-timeago"><span class="timeago" datetime="2024-04-08T15:50:38+00:00" locale="en"></span></span><span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-iso_date">2024-04-08</span>
  </span>

    
    
    
    
  </aside>





                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12Z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": ".", "features": ["navigation.top", "navigation.sections", "navigation.tabs", "navigation.tabs.sticky", "navigation.tracking", "navigation.instant", "content.code.copy"], "search": "assets/javascripts/workers/search.b8dbb3d2.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="assets/javascripts/bundle.bd41221c.min.js"></script>
      
        <script src="js/timeago.min.js"></script>
      
        <script src="js/timeago_mkdocs_material.js"></script>
      
    
  </body>
</html>